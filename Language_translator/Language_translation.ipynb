{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9da533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/pasindumalinda/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from local path: /Users/pasindumalinda/Downloads/nllb-200-1.3B...\n",
      "Tokenizer loaded successfully!\n",
      "Loading model from local path: /Users/pasindumalinda/Downloads/nllb-200-1.3B...\n",
      "Using device: cpu\n",
      "Model loaded successfully!\n",
      "Adding language code mapping to tokenizer...\n",
      "Setting up fallback translator (Helsinki-NLP)...\n",
      "Fallback translator not available: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "Translator initialized successfully\n",
      "Reading CSV file: /Volumes/KODAK/folder 02/language_translation/Language_translator/data/filtered_dataset.csv\n",
      "Found 807 rows to translate\n",
      "Backup file english_translated_comments.csv.bak already exists, not overwriting it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83a8b851ed34895961e1ff2cc64949a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating batches:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c7b311bceb42819b9e8c419e4a4088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 1:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation interrupted. Saving partial results...\n",
      "Translation statistics: {'total_rows': 807, 'error_count': 0, 'success_rate': 100.0, 'problem_translations': 1, 'good_translations': 0}\n",
      "Translation completed! Results saved to 'english_translated_comments.csv'\n",
      "\n",
      "Sample of translations:\n",
      "\n",
      "Reviewing 5 random samples:\n",
      "Sample 1:\n",
      "Original: රුපියල් 700ට මොන කාඩ්ද හලෝ ඒ මනුස්සයට උදේ පාන්දරම cash වලින් දුන්නනම් ඉවරයිනෙ මමත් කැමතිම නිලියක් තමයි ඔයා ඒත් මේකනම් මහ අසික්කිත වනචාරි වැඩක් ඔයා අනිවාර්යයෙන් මේ වවීඩියෝ එක අයින් කරගනී ඔයාට ඔයාගෙ වැරැද්ද තේරුනාම ඒත් මේකෙන් ඔයා ඔයාටම කරගත්ත ඩැමේජ් එක හැමදාටම තියෙයි\n",
      "Translation: \n",
      "Confidence: 0.00\n",
      "--------------------------------------------------------------------------------\n",
      "Sample 2:\n",
      "Original: දුවන්ද කට්ටියට විතරි දන්නවා සල්ලි කොච්චරක් හම්බු කරුත් එකේ card payment දෙමමුත් අර සල්ලි ඉතුරු වෙන්න නෙහ ඔක්කොම companya කපාගන්නවා එකී වෙඩිම කට්ටිය කෙමත්තක් වෙන්න නෙහ customers ලා ට එක හොදි but ද්රිවෙර්ස් ලා ට එක ගොඩක් වෙලාව එක හැමවෙන්න නෙහ\n",
      "Translation: \n",
      "Confidence: 0.00\n",
      "--------------------------------------------------------------------------------\n",
      "Sample 3:\n",
      "Original: waradda tiyenne pickme eke mokada hetuwa card walin apuwama drive ge atata salli enne nha pick me wage ayata adika laba upayanwa kalin mehema prana tibune nha the cash the kiyala driver ahuwe nha mulika waradda pick me eke adika laba upayanawa\n",
      "Translation: \n",
      "Confidence: 0.00\n",
      "--------------------------------------------------------------------------------\n",
      "Sample 4:\n",
      "Original: II අවුරුදු හතරක් පික්මී දුවලා තියෙනවා කස්ටමර් කම්ප්ලේන් නෑ ජිවිතේට කාඩ්ද කෑෂ්ද අහලා නෑ මේයා සද්දේ දාන ටික වීඩියෝ එකේ නෑ මුලින් කෑෂ් හයර් එකක් කියලා මගදී කාඩ් එක දාලා තමයි මේ බැදුම් අහන්නේ හැබැයි මෙහෙමයි එක වෙලාවකට ඇඩිලා තියෙනවා මේ කාඩ් හයර් නිසා කාඩ් උනත් කෑෂ් උනත් සල්ලි සල්ලිම තමයි සමහර වෙලාවට පික්මී එකට සල්ලි ගෙවන්න තිබුනොත් කාඩ් හයර්ම තමයි වැටෙන්නේ එහෙම වෙලාවට ඔක්කොම කැපිලා තෙල් ගහන්නවත්කන්නවත් සල්ලි නැතුව අසරණවෙන වාර අනන්තයි ලොකුවට සල්ලි අතේ තියාගන්න අමාරුයි මේ රස්සාවේවාහනේ හදන්නෆිනෑස් ගෙවන්න ගෙදර වියදම් 14 විතර පික්මී එකට ගෙවන්නත් වෙනවා සමහර කඩ තියෙනවා නේද කාඩ් පේමට් ගන්නේ නැති ඒවගේ තැනකට ගියාම කාඩ් ක පුපේ ගහන් සල්ලි දෙනව නේද ඒ වගේම රියදුරු අකමැති නම් කාඩ් හයර් එකට කැන්සල් කරලා වෙන එකක යන්න පුළුවන් උගේ වාහනේම ඉදලා සද්දේ දාන්නේ නැතුව\n",
      "Translation: \n",
      "Confidence: 0.00\n",
      "--------------------------------------------------------------------------------\n",
      "Sample 5:\n",
      "Original: කාඩ් බැයිනම් pickme එකෙන් අයින් වෙලා නිකං හයර් දුවපං හුත්තෝ\n",
      "Translation: \n",
      "Confidence: 0.00\n",
      "--------------------------------------------------------------------------------\n",
      "Saved 1 problematic translations to 'problem_translations.csv' for review\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK resources for sentence splitting\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    \n",
    "# Download additional NLTK resources that may help with text processing\n",
    "try:\n",
    "    nltk.data.find('tokenizers/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "class SinhalaEnglishTranslator:\n",
    "    def __init__(self, model_path= '/Users/pasindumalinda/Downloads/nllb-200-1.3B'):\n",
    "        \"\"\"Initialize the translator with the specified model\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Local path to the model directory\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.src_lang = \"sin_Sinh\"  # Sinhala\n",
    "        self.tgt_lang = \"eng_Latn\"  # English\n",
    "        \n",
    "        print(f\"Loading tokenizer from local path: {model_path}...\")\n",
    "        \n",
    "        # First check if config.json exists, if not create a minimal one\n",
    "        config_path = os.path.join(model_path, \"config.json\")\n",
    "        if not os.path.exists(config_path):\n",
    "            print(\"Config file not found. Creating minimal config.json...\")\n",
    "            import json\n",
    "            config = {\n",
    "                \"model_type\": \"nllb\",\n",
    "                \"architectures\": [\"NllbForConditionalGeneration\"],\n",
    "                \"_name_or_path\": \"facebook/nllb-200-1.3B\"\n",
    "            }\n",
    "            os.makedirs(model_path, exist_ok=True)\n",
    "            with open(config_path, 'w') as f:\n",
    "                json.dump(config, f)\n",
    "            print(f\"Created config.json at {config_path}\")\n",
    "        \n",
    "        # Try to load the tokenizer\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading local tokenizer: {e}\")\n",
    "            print(\"Falling back to downloading from HuggingFace...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-1.3B\")\n",
    "            # Save tokenizer to local path\n",
    "            self.tokenizer.save_pretrained(model_path)\n",
    "            print(f\"Saved tokenizer to {model_path}\")\n",
    "        \n",
    "        print(\"Tokenizer loaded successfully!\")\n",
    "        \n",
    "        print(f\"Loading model from local path: {model_path}...\")\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Try to load the model\n",
    "        try:\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path, trust_remote_code=True)\n",
    "            self.model = self.model.to(self.device)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading local model: {e}\")\n",
    "            print(\"Falling back to downloading from HuggingFace...\")\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-1.3B\")\n",
    "            self.model = self.model.to(self.device)\n",
    "            # Save model to local path\n",
    "            self.model.save_pretrained(model_path)\n",
    "            print(f\"Saved model to {model_path}\")\n",
    "        \n",
    "        print(\"Model loaded successfully!\")\n",
    "        \n",
    "        # Ensure we have language code mapping for NLLB models\n",
    "        # This is essential for the forced_bos_token_id in translation\n",
    "        if not hasattr(self.tokenizer, 'lang_code_to_id'):\n",
    "            print(\"Adding language code mapping to tokenizer...\")\n",
    "            self.tokenizer.lang_code_to_id = {\n",
    "                \"sin_Sinh\": 50264,  # Sinhala\n",
    "                \"eng_Latn\": 128022,  # English\n",
    "                # Add more languages as needed\n",
    "            }\n",
    "        \n",
    "        # Create a fallback pipeline using a different model for challenging cases\n",
    "        try:\n",
    "            print(\"Setting up fallback translator (Helsinki-NLP)...\")\n",
    "            self.fallback_translator = pipeline(\n",
    "                \"translation\", \n",
    "                model=\"Helsinki-NLP/opus-mt-si-en\",\n",
    "                device=0 if torch.cuda.is_available() else -1\n",
    "            )\n",
    "            self.has_fallback = True\n",
    "        except Exception as e:\n",
    "            print(f\"Fallback translator not available: {e}\")\n",
    "            self.has_fallback = False\n",
    "        \n",
    "        # Common Sinhala-English mixed phrases that should be preserved\n",
    "        self.special_phrases = [\n",
    "            \"Kohomada\", \"karanna oni\", \"api\", \"mama\", \"oya\", \"eyala\",\n",
    "            \"ayubowan\", \"istuti\", \"bohoma istuti\", \"karunakara\", \n",
    "            \"oba\", \"mage\", \"samahara\", \"kawadada\", \"koheda\", \n",
    "            \"mokada\", \"ai\", \"kawuru\", \"kohomada\", \"monawada\", \"ATM\", \"atm\", \"bank\",\n",
    "            # Add more phrases specific to your dataset\n",
    "        ]\n",
    "        \n",
    "        # Setup a translation cache to avoid re-translating identical text\n",
    "        self.translation_cache = {}\n",
    "        \n",
    "        # Collection of problematic and good translations for analysis\n",
    "        self.problem_translations = []\n",
    "        self.good_translations = []\n",
    "        \n",
    "        print(\"Translator initialized successfully\")\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Clean and prepare text for translation\"\"\"\n",
    "        if pd.isna(text) or text is None:\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to string if not already\n",
    "        text = str(text)\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        \n",
    "        # Replace multiple spaces, newlines and tabs with single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove emojis (optional - comment out if you want to keep them)\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                                   u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes\n",
    "                                   u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                                   u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                                   u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                                   u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                                   u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        text = emoji_pattern.sub(r'', text)\n",
    "        \n",
    "        # Trim whitespace\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def identify_special_terms(self, text):\n",
    "        \"\"\"Identify terms that should be preserved in the translation\"\"\"\n",
    "        special_terms = []\n",
    "        \n",
    "        # Check for common phrases that should be preserved\n",
    "        for phrase in self.special_phrases:\n",
    "            if phrase.lower() in text.lower():\n",
    "                # Find the actual occurrence with original casing\n",
    "                matches = re.finditer(re.escape(phrase), text, re.IGNORECASE)\n",
    "                for match in matches:\n",
    "                    special_terms.append(text[match.start():match.end()])\n",
    "        \n",
    "        # Find mixed language terms (improved detection)\n",
    "        # This regex looks for words with both English and Sinhala characters\n",
    "        mixed_pattern = r'\\b[a-zA-Z]*[\\u0D80-\\u0DFF]+[a-zA-Z]*\\b|\\b[a-zA-Z]+[\\u0D80-\\u0DFF]*[a-zA-Z]*\\b'\n",
    "        mixed_terms = re.findall(mixed_pattern, text)\n",
    "        special_terms.extend(mixed_terms)\n",
    "        \n",
    "        # Find social media expressions (e.g., emoticons, hashtags)\n",
    "        social_pattern = r'#\\w+|@\\w+|:\\)|:\\(|;\\)|:D|:P|<3'\n",
    "        social_terms = re.findall(social_pattern, text)\n",
    "        special_terms.extend(social_terms)\n",
    "        \n",
    "        # Numbers with units (preserve as is)\n",
    "        number_pattern = r'\\b\\d+(?:\\.\\d+)?\\s*[a-zA-Z]+\\b'  # e.g., \"10kg\", \"5.5cm\"\n",
    "        number_terms = re.findall(number_pattern, text)\n",
    "        special_terms.extend(number_terms)\n",
    "        \n",
    "        # Find proper nouns (simplified approach - words starting with capital letters)\n",
    "        proper_noun_pattern = r'\\b[A-Z][a-zA-Z]*\\b'\n",
    "        proper_nouns = re.findall(proper_noun_pattern, text)\n",
    "        special_terms.extend(proper_nouns)\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        unique_terms = []\n",
    "        for term in special_terms:\n",
    "            if term not in unique_terms:\n",
    "                unique_terms.append(term)\n",
    "        \n",
    "        return unique_terms\n",
    "    \n",
    "    def chunk_long_text(self, text, max_length=512):\n",
    "        \"\"\"Break long text into manageable chunks for translation\"\"\"\n",
    "        if len(text) <= max_length:\n",
    "            return [text]\n",
    "        \n",
    "        # Try to split by sentences first\n",
    "        sentences = sent_tokenize(text)\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            # Check if adding this sentence would exceed max_length\n",
    "            if len(current_chunk) + len(sentence) + 1 <= max_length:  # +1 for the space\n",
    "                current_chunk += \" \" + sentence if current_chunk else sentence\n",
    "            else:\n",
    "                # If current chunk is not empty, add it to chunks\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk)\n",
    "                \n",
    "                # If sentence itself is too long, split it by words\n",
    "                if len(sentence) > max_length:\n",
    "                    words = sentence.split()\n",
    "                    current_chunk = \"\"\n",
    "                    for word in words:\n",
    "                        if len(current_chunk) + len(word) + 1 <= max_length:  # +1 for the space\n",
    "                            current_chunk += \" \" + word if current_chunk else word\n",
    "                        else:\n",
    "                            chunks.append(current_chunk)\n",
    "                            current_chunk = word\n",
    "                else:\n",
    "                    # Sentence is shorter than max_length but doesn't fit in current chunk\n",
    "                    current_chunk = sentence\n",
    "        \n",
    "        # Don't forget to add the last chunk if it's not empty\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def translate_with_fallback(self, chunk):\n",
    "        \"\"\"Attempt translation with fallback model if available\"\"\"\n",
    "        if not self.has_fallback:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # The Helsinki-NLP model uses a different approach\n",
    "            result = self.fallback_translator(chunk, max_length=512)\n",
    "            if result and isinstance(result, list) and len(result) > 0:\n",
    "                return result[0]['translation_text']\n",
    "        except Exception as e:\n",
    "            print(f\"Fallback translation error: {e}\")\n",
    "        return None\n",
    "    \n",
    "    def translate_text(self, text, max_length=512):\n",
    "        \"\"\"Translate text from Sinhala to English with special term handling\"\"\"\n",
    "        if not text or pd.isna(text) or len(text.strip()) == 0:\n",
    "            return \"\"\n",
    "        \n",
    "        # Check cache first\n",
    "        if text in self.translation_cache:\n",
    "            return self.translation_cache[text]\n",
    "        \n",
    "        # Preprocess\n",
    "        text = self.preprocess_text(text)\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Identify special terms\n",
    "        special_terms = self.identify_special_terms(text)\n",
    "        \n",
    "        # Create placeholders for special terms\n",
    "        placeholders = {}\n",
    "        for i, term in enumerate(special_terms):\n",
    "            placeholder = f\"__SPECIAL_TERM_{i}__\"\n",
    "            placeholders[placeholder] = term\n",
    "            text = text.replace(term, placeholder)\n",
    "        \n",
    "        # Handle long text by chunking\n",
    "        chunks = self.chunk_long_text(text, max_length)\n",
    "        translated_chunks = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            # Skip empty chunks\n",
    "            if not chunk.strip():\n",
    "                continue\n",
    "                \n",
    "            # Add source language tag to the input text for NLLB model\n",
    "            tagged_chunk = f\">>{self.src_lang}<< {chunk}\"\n",
    "            \n",
    "            try:\n",
    "                # Tokenize\n",
    "                inputs = self.tokenizer(tagged_chunk, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                \n",
    "                # Get the forced BOS token ID for the target language\n",
    "                try:\n",
    "                    forced_bos_token_id = self.tokenizer.lang_code_to_id[self.tgt_lang]\n",
    "                except (KeyError, AttributeError):\n",
    "                    # Fallback for older tokenizers that don't have lang_code_to_id\n",
    "                    print(\"Using default forced_bos_token_id for English\")\n",
    "                    forced_bos_token_id = 128022  # Default ID for English in NLLB\n",
    "                \n",
    "                # Translate with primary model\n",
    "                with torch.no_grad():\n",
    "                    translated_tokens = self.model.generate(\n",
    "                        **inputs,\n",
    "                        forced_bos_token_id=forced_bos_token_id,\n",
    "                        max_length=max_length,\n",
    "                        num_beams=5,  # You can increase this for higher quality but slower translation\n",
    "                        num_return_sequences=1,\n",
    "                        length_penalty=1.0,\n",
    "                        early_stopping=True\n",
    "                    )\n",
    "                \n",
    "                # Decode\n",
    "                translation = self.tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "                \n",
    "                # Quality check - if translation seems problematic (very short or unchanged),\n",
    "                # try fallback model if available\n",
    "                if (len(translation) < 10 and len(chunk) > 20) or translation == chunk:\n",
    "                    fallback_translation = self.translate_with_fallback(chunk)\n",
    "                    if fallback_translation:\n",
    "                        translation = fallback_translation\n",
    "                        print(f\"Used fallback for chunk: {chunk[:30]}...\")\n",
    "                \n",
    "                translated_chunks.append(translation)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error translating chunk: {e}\")\n",
    "                # Try fallback model on error\n",
    "                fallback_translation = self.translate_with_fallback(chunk)\n",
    "                if fallback_translation:\n",
    "                    translated_chunks.append(fallback_translation)\n",
    "                else:\n",
    "                    # If all fails, add original chunk\n",
    "                    translated_chunks.append(chunk)\n",
    "                    print(f\"Failed to translate chunk: {chunk[:50]}...\")\n",
    "        \n",
    "        # Combine chunks\n",
    "        translation = \" \".join(translated_chunks)\n",
    "        \n",
    "        # Replace placeholders with original terms\n",
    "        for placeholder, term in placeholders.items():\n",
    "            translation = translation.replace(placeholder, term)\n",
    "        \n",
    "        # Apply post-processing\n",
    "        translation = self.post_process_translation(translation)\n",
    "        \n",
    "        # Cache the translation\n",
    "        self.translation_cache[text] = translation\n",
    "        \n",
    "        # Collect statistics (optional)\n",
    "        if len(translation.split()) < len(text.split()) / 3:\n",
    "            self.problem_translations.append((text, translation))\n",
    "        elif len(translation.split()) > 5:\n",
    "            self.good_translations.append((text, translation))\n",
    "        \n",
    "        return translation\n",
    "    \n",
    "    def post_process_translation(self, translation):\n",
    "        \"\"\"Apply post-processing to improve translation quality\"\"\"\n",
    "        if pd.isna(translation):\n",
    "            return \"\"\n",
    "        \n",
    "        # Fix common translation artifacts\n",
    "        translation = re.sub(r'\\s+', ' ', translation)  # Remove multiple spaces\n",
    "        translation = translation.strip()\n",
    "        \n",
    "        # Fix common punctuation issues\n",
    "        translation = re.sub(r'\\s+([.,;:!?])', r'\\1', translation)  # No space before punctuation\n",
    "        translation = re.sub(r'([.,;:!?])([^\\s])', r'\\1 \\2', translation)  # Space after punctuation\n",
    "        \n",
    "        # Fix capitalization\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', translation)\n",
    "        sentences = [s.capitalize() for s in sentences if s]\n",
    "        translation = ' '.join(sentences)\n",
    "        \n",
    "        # Fix common translation errors (you can expand this list)\n",
    "        common_errors = {\n",
    "            # Common translation errors from Sinhala to English\n",
    "            \"the the\": \"the\",\n",
    "            \"a the\": \"the\",\n",
    "            \"an the\": \"the\",\n",
    "            # Add more errors that you observe in your translations\n",
    "        }\n",
    "        \n",
    "        for error, correction in common_errors.items():\n",
    "            translation = re.sub(r'\\b' + re.escape(error) + r'\\b', correction, translation, flags=re.IGNORECASE)\n",
    "        \n",
    "        return translation\n",
    "    \n",
    "    def batch_translate(self, df, column_name, batch_size=16, output_column='english_translation'):\n",
    "        \"\"\"Translate a dataframe column in batches with progress tracking and error handling\"\"\"\n",
    "        total_rows = len(df)\n",
    "        df_result = df.copy()\n",
    "        \n",
    "        # Add a new column for translations if it doesn't exist\n",
    "        if output_column not in df_result.columns:\n",
    "            df_result[output_column] = \"\"\n",
    "        \n",
    "        # Add error tracking column\n",
    "        error_column = 'translation_error'\n",
    "        if error_column not in df_result.columns:\n",
    "            df_result[error_column] = False\n",
    "        \n",
    "        # Add translation confidence scores (based on simple heuristics)\n",
    "        confidence_column = 'translation_confidence'\n",
    "        if confidence_column not in df_result.columns:\n",
    "            df_result[confidence_column] = 0.0\n",
    "        \n",
    "        error_count = 0\n",
    "        \n",
    "        try:\n",
    "            for i in tqdm(range(0, total_rows, batch_size), desc=\"Translating batches\"):\n",
    "                end_idx = min(i + batch_size, total_rows)\n",
    "                batch = df.iloc[i:end_idx]\n",
    "                \n",
    "                for idx, row in tqdm(batch.iterrows(), desc=f\"Batch {i//batch_size + 1}\", leave=False, total=len(batch)):\n",
    "                    try:\n",
    "                        text = row[column_name]\n",
    "                        \n",
    "                        # Skip translation if the text is empty\n",
    "                        if pd.isna(text) or not str(text).strip():\n",
    "                            df_result.at[idx, output_column] = \"\"\n",
    "                            df_result.at[idx, confidence_column] = 0.0\n",
    "                            continue\n",
    "                        \n",
    "                        # Translate\n",
    "                        translation = self.translate_text(text)\n",
    "                        \n",
    "                        # Set translation\n",
    "                        df_result.at[idx, output_column] = translation\n",
    "                        df_result.at[idx, error_column] = False\n",
    "                        \n",
    "                        # Calculate a rough confidence score based on heuristics\n",
    "                        src_words = len(str(text).split())\n",
    "                        tgt_words = len(translation.split())\n",
    "                        \n",
    "                        # If target is too short compared to source, lower confidence\n",
    "                        if src_words > 3 and tgt_words < src_words / 3:\n",
    "                            confidence = 0.3\n",
    "                        # If lengths are somewhat proportional, higher confidence\n",
    "                        elif tgt_words > 0 and 0.5 <= (tgt_words / max(1, src_words)) <= 2.0:\n",
    "                            confidence = 0.8\n",
    "                        else:\n",
    "                            confidence = 0.5\n",
    "                            \n",
    "                        df_result.at[idx, confidence_column] = confidence\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error translating row {idx}: {e}\")\n",
    "                        df_result.at[idx, error_column] = True\n",
    "                        error_count += 1\n",
    "                    \n",
    "                    # Add small delay to avoid potential rate limiting or overheating\n",
    "                    time.sleep(0.05)\n",
    "                \n",
    "                # Save intermediate results every batch\n",
    "                if i > 0 and i % (batch_size * 5) == 0:\n",
    "                    temp_file = f\"translation_checkpoint_{i}.csv\"\n",
    "                    df_result.to_csv(temp_file, index=False)\n",
    "                    print(f\"Checkpoint saved: {temp_file}\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Translation interrupted. Saving partial results...\")\n",
    "        \n",
    "        if error_count > 0:\n",
    "            print(f\"Completed with {error_count} errors out of {total_rows} rows.\")\n",
    "        \n",
    "        # Generate summary statistics\n",
    "        translation_stats = {\n",
    "            \"total_rows\": total_rows,\n",
    "            \"error_count\": error_count,\n",
    "            \"success_rate\": (total_rows - error_count) / total_rows * 100,\n",
    "            \"problem_translations\": len(self.problem_translations),\n",
    "            \"good_translations\": len(self.good_translations)\n",
    "        }\n",
    "        \n",
    "        print(f\"Translation statistics: {translation_stats}\")\n",
    "        \n",
    "        return df_result\n",
    "    \n",
    "    def review_sample(self, df, original_column, translation_column, n=5):\n",
    "        \"\"\"Display a sample of translations for manual review\"\"\"\n",
    "        if n > len(df):\n",
    "            n = len(df)\n",
    "            \n",
    "        print(f\"\\nReviewing {n} random samples:\")\n",
    "        sample = df.sample(n)\n",
    "        for i, (_, row) in enumerate(sample.iterrows()):\n",
    "            print(f\"Sample {i+1}:\")\n",
    "            print(f\"Original: {row[original_column]}\")\n",
    "            print(f\"Translation: {row[translation_column]}\")\n",
    "            \n",
    "            # Show confidence if available\n",
    "            if 'translation_confidence' in row:\n",
    "                print(f\"Confidence: {row['translation_confidence']:.2f}\")\n",
    "                \n",
    "            print(\"-\" * 80)\n",
    "    \n",
    "    def translate_file(self, input_file, output_file, comment_column, batch_size=16):\n",
    "        \"\"\"Process an entire CSV file and save the results with error handling\"\"\"\n",
    "        print(f\"Reading CSV file: {input_file}\")\n",
    "        \n",
    "        try:\n",
    "            # Try to read with UTF-8 encoding first\n",
    "            df = pd.read_csv(input_file, encoding='utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            # If that fails, try with the more permissive ISO-8859-1 encoding\n",
    "            print(\"UTF-8 encoding failed, trying ISO-8859-1...\")\n",
    "            df = pd.read_csv(input_file, encoding='ISO-8859-1')\n",
    "        \n",
    "        print(f\"Found {len(df)} rows to translate\")\n",
    "        \n",
    "        # Check if the comment column exists\n",
    "        if comment_column not in df.columns:\n",
    "            # Show available columns to help user\n",
    "            print(f\"Available columns: {', '.join(df.columns)}\")\n",
    "            raise ValueError(f\"Column '{comment_column}' not found in the CSV file\")\n",
    "        \n",
    "        # Create backup of original file if we're going to overwrite it\n",
    "        if os.path.exists(output_file) and input_file != output_file:\n",
    "            backup_file = output_file + '.bak'\n",
    "            if os.path.exists(backup_file):\n",
    "                print(f\"Backup file {backup_file} already exists, not overwriting it.\")\n",
    "            else:\n",
    "                import shutil\n",
    "                shutil.copy2(output_file, backup_file)\n",
    "                print(f\"Created backup of existing output file: {backup_file}\")\n",
    "        \n",
    "        # Translate\n",
    "        df_translated = self.batch_translate(df, comment_column, batch_size)\n",
    "        \n",
    "        # Save results\n",
    "        df_translated.to_csv(output_file, index=False)\n",
    "        print(f\"Translation completed! Results saved to '{output_file}'\")\n",
    "        \n",
    "        # Show a sample of the results\n",
    "        print(\"\\nSample of translations:\")\n",
    "        self.review_sample(df_translated, comment_column, 'english_translation', 5)\n",
    "        \n",
    "        # Offer to save problem translations for review\n",
    "        if self.problem_translations:\n",
    "            problem_file = \"problem_translations.csv\"\n",
    "            pd.DataFrame(self.problem_translations, columns=['original', 'translation']).to_csv(problem_file, index=False)\n",
    "            print(f\"Saved {len(self.problem_translations)} problematic translations to '{problem_file}' for review\")\n",
    "        \n",
    "        return df_translated\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the translator with the local model\n",
    "    model_path = '/Users/pasindumalinda/Downloads/nllb-200-1.3B'\n",
    "    translator = SinhalaEnglishTranslator(model_path=model_path)\n",
    "    \n",
    "    # Set your file paths and column name\n",
    "    input_csv = \"/Volumes/KODAK/folder 02/language_translation/Language_translator/data/filtered_dataset.csv\"\n",
    "    output_csv = \"english_translated_comments.csv\"\n",
    "    comment_column = \"Comment\"  \n",
    "    \n",
    "    # Process the file with smaller batch size for more frequent updates\n",
    "    translated_df = translator.translate_file(input_csv, output_csv, comment_column, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9669be",
   "metadata": {},
   "source": [
    "# Second Try using small model 'Helsinki-NLP/opus-mt-si-en'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b53e815",
   "metadata": {},
   "source": [
    "## Step 1: Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1607dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0800a1",
   "metadata": {},
   "source": [
    "## Step 2: Loading and exploring the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ed302e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file loaded successfully.\n",
      "Shape of the data: (807, 1)\n",
      "Column names: ['Comment']\n",
      "\n",
      "First few rows of the data:\n",
      "                                             Comment\n",
      "0  මොනා උනත් පොත්ත සුදු කෑල්ලක් දැකලා දෙකට නැවුනෙ...\n",
      "1  බැරිනං PickMe එකෙන් අයින් වෙලා නිකං හයර් දුවපං...\n",
      "2  ට් ‍ රිප් එක මැදදී කෑශ් හයර් එක කාඩ් හයර් එකට ...\n",
      "3  me දුවපු කොල්ලෙක්ට විතරයි seen එක තේරෙන්නෙඋදේම...\n",
      "4  මාත් හයර් දුවන්නෙ කස්ටමර්ගෙ පැත්තෙන් බලනකොට සා...\n",
      "\n",
      "Missing values in each column:\n",
      "Comment    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_and_explore_csv(file_path):\n",
    "    \"\"\"\n",
    "    Load the CSV file and explore its structure.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file containing Sinhala comments\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame containing the CSV data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        # Note: You might need to adjust parameters based on your CSV structure\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        \n",
    "        # Display basic information\n",
    "        print(f\"CSV file loaded successfully.\")\n",
    "        print(f\"Shape of the data: {df.shape}\")\n",
    "        print(f\"Column names: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Display first few rows\n",
    "        print(\"\\nFirst few rows of the data:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Check for missing values\n",
    "        print(\"\\nMissing values in each column:\")\n",
    "        print(df.isnull().sum())\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "df = load_and_explore_csv('/Volumes/KODAK/folder 02/language_translation/Language_translator/data/filtered_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd338791",
   "metadata": {},
   "source": [
    "## Step 3: Setting up the translation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514345ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "def setup_translation_model():\n",
    "    \"\"\"\n",
    "    Set up the Facebook NLLB model for Sinhala to English translation.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, tokenizer, device) - The loaded model, tokenizer, and device\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define model name\n",
    "        model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "        \n",
    "        # Load tokenizer and model\n",
    "        print(f\"Loading the tokenizer for {model_name}...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        print(f\"Loading the model {model_name}...\")\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\",  # Automatically handles GPU placement\n",
    "            torch_dtype=torch.float16  # Uses less memory\n",
    "        )\n",
    "        \n",
    "        # Check if GPU is available\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Model loaded successfully and running on {device}\")\n",
    "        \n",
    "        return model, tokenizer, device\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up translation model: {e}\")\n",
    "        print(\"Try running: pip install --upgrade transformers flash-attn --no-build-isolation\")\n",
    "        return None, None, None\n",
    "\n",
    "# Example usage\n",
    "# model, tokenizer, device = setup_translation_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901128aa",
   "metadata": {},
   "source": [
    "## Step 4: Creating the translation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87eafc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Translate a Sinhala text to English using the loaded model and tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The Sinhala text to translate\n",
    "        model: The MarianMT translation model\n",
    "        tokenizer: The MarianTokenizer\n",
    "        device: The device (CPU/GPU) to use for translation\n",
    "        \n",
    "    Returns:\n",
    "        str: The translated English text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Skip translation if text is empty or None\n",
    "        if not text or pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Tokenize the text\n",
    "        batch = tokenizer([text], return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Move input tensors to the device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "        # Generate translation\n",
    "        translated = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=512,  # Adjust this based on your needs\n",
    "            num_beams=4,     # Beam search for better translations\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        # Decode the translated output\n",
    "        translated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)[0]\n",
    "        \n",
    "        return translated_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error translating text: {e}\")\n",
    "        print(f\"Problematic text: {text}\")\n",
    "        return f\"ERROR: {str(e)}\"\n",
    "\n",
    "# Example usage\n",
    "# translated = translate_text(\"අසරන මනුස්සයෙක්ගෙ දවසම කාලා\", model, tokenizer, device)\n",
    "# print(translated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edda6a18",
   "metadata": {},
   "source": [
    "## Step 5: Batch processing function for translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6ce3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_dataframe(df, text_column, model, tokenizer, device, batch_size=10):\n",
    "    \"\"\"\n",
    "    Translate all texts in a specific column of a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing the text to translate\n",
    "        text_column (str): The name of the column containing Sinhala text\n",
    "        model: The MarianMT translation model\n",
    "        tokenizer: The MarianTokenizer\n",
    "        device: The device (CPU/GPU) to use for translation\n",
    "        batch_size (int): Number of translations to process before showing progress\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with an additional column containing translations\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Add a new column for translations\n",
    "    result_df['translated_text'] = \"\"\n",
    "    \n",
    "    # Get total number of rows\n",
    "    total_rows = len(df)\n",
    "    print(f\"Starting translation of {total_rows} rows...\")\n",
    "    \n",
    "    # Track time\n",
    "    start_time = time.time()\n",
    "    last_update_time = start_time\n",
    "    \n",
    "    # Process each row\n",
    "    for i, row in df.iterrows():\n",
    "        # Get the text to translate\n",
    "        text = str(row[text_column])\n",
    "        \n",
    "        # Translate the text\n",
    "        translated = translate_text(text, model, tokenizer, device)\n",
    "        \n",
    "        # Store the translation\n",
    "        result_df.at[i, 'translated_text'] = translated\n",
    "        \n",
    "        # Show progress every batch_size rows\n",
    "        if (i + 1) % batch_size == 0 or (i + 1) == total_rows:\n",
    "            current_time = time.time()\n",
    "            elapsed = current_time - start_time\n",
    "            batch_elapsed = current_time - last_update_time\n",
    "            last_update_time = current_time\n",
    "            \n",
    "            progress = (i + 1) / total_rows * 100\n",
    "            rows_per_sec = batch_size / batch_elapsed if batch_elapsed > 0 else 0\n",
    "            \n",
    "            # Estimate remaining time\n",
    "            remaining_rows = total_rows - (i + 1)\n",
    "            eta_seconds = remaining_rows / rows_per_sec if rows_per_sec > 0 else 0\n",
    "            eta_min = eta_seconds / 60\n",
    "            \n",
    "            print(f\"Progress: {i+1}/{total_rows} ({progress:.2f}%) - \"\n",
    "                  f\"Speed: {rows_per_sec:.2f} rows/sec - \"\n",
    "                  f\"Elapsed: {elapsed/60:.2f} min - \"\n",
    "                  f\"ETA: {eta_min:.2f} min\")\n",
    "            \n",
    "            # Show a sample of the translation for verification\n",
    "            if i < 5:  # Show only for first few rows\n",
    "                print(f\"Original: {text[:100]}...\")\n",
    "                print(f\"Translated: {translated[:100]}...\")\n",
    "                print(\"-\" * 50)\n",
    "    \n",
    "    print(f\"Translation completed. Total time: {(time.time() - start_time)/60:.2f} minutes\")\n",
    "    return result_df\n",
    "\n",
    "# Example usage\n",
    "# translated_df = translate_dataframe(df, 'comment_column', model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004cca15",
   "metadata": {},
   "source": [
    "## Step 6: Handling mixed language content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68c97fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def detect_language_mix(text):\n",
    "    \"\"\"\n",
    "    Detect if the text contains significant amounts of English already.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to check\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the text contains significant English, False otherwise\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return False\n",
    "    \n",
    "    # Count English words (simple heuristic)\n",
    "    # English words typically use Latin script\n",
    "    english_pattern = r'[a-zA-Z]+\\s*[a-zA-Z]*'\n",
    "    english_words = re.findall(english_pattern, text)\n",
    "    \n",
    "    # Count words that are likely Sinhala\n",
    "    # Sinhala Unicode range: U+0D80 to U+0DFF\n",
    "    sinhala_pattern = r'[\\u0D80-\\u0DFF]+\\s*[\\u0D80-\\u0DFF]*'\n",
    "    sinhala_words = re.findall(sinhala_pattern, text)\n",
    "    \n",
    "    # If no words are found, return False\n",
    "    if not english_words and not sinhala_words:\n",
    "        return False\n",
    "    \n",
    "    # Calculate the percentage of English words\n",
    "    total_words = len(english_words) + len(sinhala_words)\n",
    "    english_percentage = len(english_words) / total_words if total_words > 0 else 0\n",
    "    \n",
    "    # If more than 70% of words are English, consider it mixed/mostly English\n",
    "    return english_percentage > 0.7\n",
    "\n",
    "def smart_translate(text, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Intelligently translate text, handling mixed language content.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to translate\n",
    "        model: The MarianMT translation model\n",
    "        tokenizer: The MarianTokenizer\n",
    "        device: The device (CPU/GPU) to use for translation\n",
    "        \n",
    "    Returns:\n",
    "        str: The translated text\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Check if the text is already mostly English\n",
    "    if detect_language_mix(text):\n",
    "        # Text is already mostly English, no need for translation\n",
    "        return text\n",
    "    \n",
    "    # Text is primarily Sinhala, translate it\n",
    "    return translate_text(text, model, tokenizer, device)\n",
    "\n",
    "# Modify the dataframe translation function to use smart_translate\n",
    "def smart_translate_dataframe(df, text_column, model, tokenizer, device, batch_size=10):\n",
    "    \"\"\"\n",
    "    Smartly translate all texts in a specific column of a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing the text to translate\n",
    "        text_column (str): The name of the column containing Sinhala text\n",
    "        model: The MarianMT translation model\n",
    "        tokenizer: The MarianTokenizer\n",
    "        device: The device (CPU/GPU) to use for translation\n",
    "        batch_size (int): Number of translations to process before showing progress\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with an additional column containing translations\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Add a new column for translations\n",
    "    result_df['translated_text'] = \"\"\n",
    "    \n",
    "    # Get total number of rows\n",
    "    total_rows = len(df)\n",
    "    print(f\"Starting smart translation of {total_rows} rows...\")\n",
    "    \n",
    "    # Track time\n",
    "    start_time = time.time()\n",
    "    last_update_time = start_time\n",
    "    \n",
    "    # Counters for statistics\n",
    "    translated_count = 0\n",
    "    already_english_count = 0\n",
    "    \n",
    "    # Process each row\n",
    "    for i, row in df.iterrows():\n",
    "        # Get the text to translate\n",
    "        text = str(row[text_column])\n",
    "        \n",
    "        # Check if already mostly English\n",
    "        is_mostly_english = detect_language_mix(text)\n",
    "        \n",
    "        if is_mostly_english:\n",
    "            # Keep as is\n",
    "            result_df.at[i, 'translated_text'] = text\n",
    "            already_english_count += 1\n",
    "        else:\n",
    "            # Translate the text\n",
    "            translated = translate_text(text, model, tokenizer, device)\n",
    "            result_df.at[i, 'translated_text'] = translated\n",
    "            translated_count += 1\n",
    "        \n",
    "        # Show progress every batch_size rows\n",
    "        if (i + 1) % batch_size == 0 or (i + 1) == total_rows:\n",
    "            current_time = time.time()\n",
    "            elapsed = current_time - start_time\n",
    "            batch_elapsed = current_time - last_update_time\n",
    "            last_update_time = current_time\n",
    "            \n",
    "            progress = (i + 1) / total_rows * 100\n",
    "            rows_per_sec = batch_size / batch_elapsed if batch_elapsed > 0 else 0\n",
    "            \n",
    "            # Estimate remaining time\n",
    "            remaining_rows = total_rows - (i + 1)\n",
    "            eta_seconds = remaining_rows / rows_per_sec if rows_per_sec > 0 else 0\n",
    "            eta_min = eta_seconds / 60\n",
    "            \n",
    "            print(f\"Progress: {i+1}/{total_rows} ({progress:.2f}%) - \"\n",
    "                  f\"Speed: {rows_per_sec:.2f} rows/sec - \"\n",
    "                  f\"Elapsed: {elapsed/60:.2f} min - \"\n",
    "                  f\"ETA: {eta_min:.2f} min\")\n",
    "    \n",
    "    print(f\"Translation completed. Total time: {(time.time() - start_time)/60:.2f} minutes\")\n",
    "    print(f\"Statistics: {translated_count} texts translated, {already_english_count} already mostly English\")\n",
    "    return result_df\n",
    "\n",
    "# Example usage\n",
    "# translated_df = smart_translate_dataframe(df, 'comment_column', model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e35c71",
   "metadata": {},
   "source": [
    "## Step 7: Saving the translated data to a new CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9010f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_translated_csv(df, output_path, include_original=True):\n",
    "    \"\"\"\n",
    "    Save the DataFrame with translations to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing the original text and translations\n",
    "        output_path (str): Path to save the output CSV file\n",
    "        include_original (bool): Whether to include the original text in the output\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if saved successfully, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a copy to avoid modifying the original\n",
    "        output_df = df.copy()\n",
    "        \n",
    "        # Reorder columns to put translations at the end if needed\n",
    "        # This is just for better readability of the CSV\n",
    "        if include_original:\n",
    "            # Keep all columns but ensure translation is at the end\n",
    "            cols = [col for col in output_df.columns if col != 'translated_text'] + ['translated_text']\n",
    "            output_df = output_df[cols]\n",
    "        else:\n",
    "            # Replace the original text column with the translated text\n",
    "            # Find the column that was translated (assuming it's stored somewhere)\n",
    "            # For now, we'll just keep all columns including translated_text\n",
    "            pass\n",
    "        \n",
    "        # Save to CSV\n",
    "        output_df.to_csv(output_path, index=False, encoding='utf-8-sig')  # utf-8-sig includes BOM for Excel compatibility\n",
    "        \n",
    "        print(f\"Translated data saved to {output_path}\")\n",
    "        print(f\"Total rows saved: {len(output_df)}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving translated CSV: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example usage\n",
    "# success = save_translated_csv(translated_df, 'translated_comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e496100",
   "metadata": {},
   "source": [
    "## Step 8: Error handling and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf70a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_translations(df, original_column, translated_column):\n",
    "    \"\"\"\n",
    "    Validate the translations to ensure quality.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing original and translated text\n",
    "        original_column (str): The name of the column containing original text\n",
    "        translated_column (str): The name of the column containing translated text\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing problematic translations for review\n",
    "    \"\"\"\n",
    "    # Create a copy for validation results\n",
    "    validation_df = pd.DataFrame(columns=['row_index', 'original', 'translated', 'issue'])\n",
    "    \n",
    "    # List to collect problematic rows\n",
    "    issues = []\n",
    "    \n",
    "    # Check each row\n",
    "    for i, row in df.iterrows():\n",
    "        original = str(row[original_column])\n",
    "        translated = str(row[translated_column])\n",
    "        \n",
    "        # Check for empty translations of non-empty originals\n",
    "        if original and not translated:\n",
    "            issues.append({\n",
    "                'row_index': i,\n",
    "                'original': original,\n",
    "                'translated': translated,\n",
    "                'issue': 'Empty translation'\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Check for very short translations of long originals\n",
    "        # This could indicate truncation or incomplete translation\n",
    "        if len(original) > 50 and len(translated) < 10:\n",
    "            issues.append({\n",
    "                'row_index': i,\n",
    "                'original': original,\n",
    "                'translated': translated,\n",
    "                'issue': 'Suspiciously short translation'\n",
    "            })\n",
    "            continue\n",
    "            \n",
    "        # Check for translations that contain error messages\n",
    "        if \"ERROR:\" in translated:\n",
    "            issues.append({\n",
    "                'row_index': i,\n",
    "                'original': original,\n",
    "                'translated': translated,\n",
    "                'issue': 'Contains error message'\n",
    "            })\n",
    "            continue\n",
    "    \n",
    "    # Create DataFrame from issues list\n",
    "    if issues:\n",
    "        validation_df = pd.DataFrame(issues)\n",
    "        print(f\"Found {len(issues)} potentially problematic translations\")\n",
    "    else:\n",
    "        print(\"No translation issues found\")\n",
    "    \n",
    "    return validation_df\n",
    "\n",
    "def retry_failed_translations(df, validation_df, text_column, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Retry the failed translations.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): The original DataFrame with translations\n",
    "        validation_df (DataFrame): The DataFrame containing failed translations\n",
    "        text_column (str): The name of the column containing original text\n",
    "        model: The MarianMT translation model\n",
    "        tokenizer: The MarianTokenizer\n",
    "        device: The device (CPU/GPU) to use for translation\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: The updated DataFrame with retried translations\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    if validation_df.empty:\n",
    "        print(\"No failed translations to retry\")\n",
    "        return result_df\n",
    "    \n",
    "    print(f\"Retrying {len(validation_df)} failed translations...\")\n",
    "    \n",
    "    # Retry each failed translation\n",
    "    for _, row in validation_df.iterrows():\n",
    "        row_index = row['row_index']\n",
    "        original_text = row['original']\n",
    "        \n",
    "        # Try translation with different parameters\n",
    "        try:\n",
    "            # Simple retry with same parameters\n",
    "            translated = translate_text(original_text, model, tokenizer, device)\n",
    "            \n",
    "            # Update the DataFrame\n",
    "            result_df.at[row_index, 'translated_text'] = translated\n",
    "            print(f\"Successfully retried translation for row {row_index}\")\n",
    "            print(f\"Original: {original_text[:50]}...\")\n",
    "            print(f\"New translation: {translated[:50]}...\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Still failed to translate row {row_index}: {e}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Example usage\n",
    "# validation_results = validate_translations(translated_df, 'comment_column', 'translated_text')\n",
    "# if not validation_results.empty:\n",
    "#     translated_df = retry_failed_translations(translated_df, validation_results, 'comment_column', model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88c9fcb",
   "metadata": {},
   "source": [
    "## Step 9: Main execution script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af0c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the translation process.\n",
    "    \"\"\"\n",
    "    import argparse\n",
    "    \n",
    "    # Parse command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Translate Sinhala comments to English for NLP analysis')\n",
    "    parser.add_argument('--input', type=str, required=True, help='Path to input CSV file')\n",
    "    parser.add_argument('--output', type=str, required=True, help='Path to output CSV file')\n",
    "    parser.add_argument('--column', type=str, required=True, help='Name of column containing text to translate')\n",
    "    parser.add_argument('--batch_size', type=int, default=10, help='Batch size for progress updates')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print(\"Starting translation process...\")\n",
    "    print(f\"Input file: {args.input}\")\n",
    "    print(f\"Output file: {args.output}\")\n",
    "    print(f\"Column to translate: {args.column}\")\n",
    "    \n",
    "    # Step 1: Load the CSV file\n",
    "    df = load_and_explore_csv(args.input)\n",
    "    if df is None:\n",
    "        print(\"Failed to load CSV file. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Verify that the specified column exists\n",
    "    if args.column not in df.columns:\n",
    "        print(f\"Column '{args.column}' not found in the CSV file.\")\n",
    "        print(f\"Available columns: {df.columns.tolist()}\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Set up the translation model\n",
    "    model, tokenizer, device = setup_translation_model()\n",
    "    if model is None or tokenizer is None:\n",
    "        print(\"Failed to set up translation model. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Step 3: Translate the texts\n",
    "    print(\"Starting translation process...\")\n",
    "    translated_df = smart_translate_dataframe(df, args.column, model, tokenizer, device, args.batch_size)\n",
    "    \n",
    "    # Step 4: Validate translations\n",
    "    validation_results = validate_translations(translated_df, args.column, 'translated_text')\n",
    "    \n",
    "    # Step 5: Retry failed translations if needed\n",
    "    if not validation_results.empty:\n",
    "        print(\"Retrying failed translations...\")\n",
    "        translated_df = retry_failed_translations(translated_df, validation_results, args.column, model, tokenizer, device)\n",
    "        \n",
    "        # Re-validate to check how many issues were resolved\n",
    "        new_validation = validate_translations(translated_df, args.column, 'translated_text')\n",
    "        if not new_validation.empty:\n",
    "            print(f\"After retry, {len(new_validation)} translations still have issues\")\n",
    "            print(\"Saving problematic translations for manual review...\")\n",
    "            new_validation.to_csv(args.output.replace('.csv', '_issues.csv'), index=False)\n",
    "    \n",
    "    # Step 6: Save the results\n",
    "    success = save_translated_csv(translated_df, args.output)\n",
    "    if success:\n",
    "        print(f\"Translation process completed successfully. Results saved to {args.output}\")\n",
    "    else:\n",
    "        print(\"Failed to save results.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d2136b",
   "metadata": {},
   "source": [
    "## Step 10: Test and example script for direct use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52cb9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example script to demonstrate the usage of the translation functions.\n",
    "\"\"\"\n",
    "# Import all necessary functions from the above modules\n",
    "# In a real implementation, these would be imported from properly organized modules\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Import our functions (in reality, these would be defined in the script or imported)\n",
    "# You would need to replace these imports with the actual functions from Steps 1-9\n",
    "\n",
    "# Here's a self-contained example that shows how to use the code:\n",
    "\n",
    "def example_with_sample_data():\n",
    "    \"\"\"\n",
    "    Example using the sample data provided in the requirements.\n",
    "    \"\"\"\n",
    "    # Create a sample DataFrame with the provided example\n",
    "    sample_data = pd.DataFrame({\n",
    "        'comment': [\n",
    "            'අසරන මනුස්සයෙක්ගෙ දවසම කාලා නිලිය sooting යන්නෙත් pikme weel එකක ඒකට දෙන්න මුංට සල්ලිත් නැ සල්ලි නැත්තම් atm එකකින් බැහැල කාඩ් එකෙන් සල්ලි අරන් දෙන්න තිබ්බනෙ මෙයා රටෙ මිනිස්සුන්ගෙන් මදිවට වීල් වල යන අයගෙනුත් කුණු බැනුම් අහගන්නවා',\n",
    "            'PickMe this is a serious nonsense aththatama gaman cancel karana ekai gewanna wei kiala nathnm complaint ekak dammoth ape numbers walata call aran banina ekai lata kisima safety ekak naha complaint ekak daaddi ape address ekath ekkalu yanne U all have to update ur system and choose professional quality drivers n give them strict warnings about the service u all providing',\n",
    "            'நெருக்கடியான சூழ்நிலையில். .. .. .. .. .. .. .. .. .. .. .. .. .. நெருக்கடியான சூழ்நிலையில். .. .. .. நெருக்கடியான சூழ்நிலையில். .. .. .. . நெருக்கடியான சூழ்நிலையில். .. .. .. .. .. .. .. .. .. .. . நெருக்கடிகள். .. .. .. .. .. .. .. .. .. .. . நெருக்கடியான சூழ்நிலையில். .. .. .. நெருக்கடியான சூழ்நிலையில். .. .. .. .. .. .. .. .. .. நெருக்கடியான சூழ்நிலையில். .. .. .. .. .. .. .. .. .. .'\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Save the sample data to a temporary CSV file\n",
    "    sample_csv_path = 'sample_comments.csv'\n",
    "    sample_data.to_csv(sample_csv_path, index=False)\n",
    "    \n",
    "    print(\"Sample CSV created with the provided examples.\")\n",
    "    \n",
    "    # Set up the translation model\n",
    "    print(\"Setting up the translation model...\")\n",
    "    model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Function to detect if text is mostly English\n",
    "    def detect_language_mix(text):\n",
    "        if not text or pd.isna(text):\n",
    "            return False\n",
    "        \n",
    "        english_pattern = r'[a-zA-Z]+\\s*[a-zA-Z]*'\n",
    "        english_words = re.findall(english_pattern, text)\n",
    "        \n",
    "        sinhala_pattern = r'[\\u0D80-\\u0DFF]+\\s*[\\u0D80-\\u0DFF]*'\n",
    "        sinhala_words = re.findall(sinhala_pattern, text)\n",
    "        \n",
    "        # Also check for Tamil script (since one example contains Tamil)\n",
    "        tamil_pattern = r'[\\u0B80-\\u0BFF]+\\s*[\\u0B80-\\u0BFF]*'\n",
    "        tamil_words = re.findall(tamil_pattern, text)\n",
    "        \n",
    "        non_english_words = len(sinhala_words) + len(tamil_words)\n",
    "        \n",
    "        # If no words are found, return False\n",
    "        if not english_words and non_english_words == 0:\n",
    "            return False\n",
    "        \n",
    "        # Calculate the percentage of English words\n",
    "        total_words = len(english_words) + non_english_words\n",
    "        english_percentage = len(english_words) / total_words if total_words > 0 else 0\n",
    "        \n",
    "        # If more than 70% of words are English, consider it mixed/mostly English\n",
    "        return english_percentage > 0.7\n",
    "    \n",
    "    # Function to translate text\n",
    "    def translate_text(text, model, tokenizer, device):\n",
    "        try:\n",
    "            if not text or pd.isna(text):\n",
    "                return \"\"\n",
    "            \n",
    "            # Skip translation if text contains Tamil script\n",
    "            # Since our model is only for Sinhala-English\n",
    "            if re.search(r'[\\u0B80-\\u0BFF]', text):\n",
    "                return \"[Tamil text detected - requires a different translation model]\"\n",
    "            \n",
    "            # Tokenize the text\n",
    "            batch = tokenizer([text], return_tensors=\"pt\", padding=True)\n",
    "            \n",
    "            # Move input tensors to the device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            \n",
    "            # Generate translation\n",
    "            translated = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=512,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            # Decode the translated output\n",
    "            translated_text = tokenizer.batch_decode(translated, skip_special_tokens=True)[0]\n",
    "            \n",
    "            return translated_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error translating text: {e}\")\n",
    "            print(f\"Problematic text: {text}\")\n",
    "            return f\"ERROR: {str(e)}\"\n",
    "    \n",
    "    # Function to smartly translate text\n",
    "    def smart_translate(text, model, tokenizer, device):\n",
    "        if not text or pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Check if the text is already mostly English\n",
    "        if detect_language_mix(text):\n",
    "            # Text is already mostly English, no need for translation\n",
    "            return text\n",
    "        \n",
    "        # Text is primarily Sinhala or another language, translate it\n",
    "        return translate_text(text, model, tokenizer, device)\n",
    "    \n",
    "    # Process each example\n",
    "    print(\"\\nTranslating sample texts:\")\n",
    "    for i, comment in enumerate(sample_data['comment']):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Original: {comment[:100]}...\")\n",
    "        \n",
    "        # Translate the text\n",
    "        translated = smart_translate(comment, model, tokenizer, device)\n",
    "        print(f\"Translated: {translated[:100]}...\")\n",
    "        \n",
    "        # Update the DataFrame with the translation\n",
    "        sample_data.at[i, 'translated_text'] = translated\n",
    "    \n",
    "    # Save the results\n",
    "    output_csv_path = 'translated_sample_comments.csv'\n",
    "    sample_data.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\nTranslations completed and saved to {output_csv_path}\")\n",
    "    print(\"Review of translations:\")\n",
    "    \n",
    "    # Show the final results\n",
    "    for i, row in sample_data.iterrows():\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Original: {row['comment'][:100]}...\")\n",
    "        print(f\"Translated: {row['translated_text'][:100]}...\")\n",
    "\n",
    "# Run the example\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running example with sample data...\")\n",
    "    example_with_sample_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83db888",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
