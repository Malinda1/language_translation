{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81b18f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from googletrans import Translator\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import re\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "36d37c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Comment",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "238f8974-086c-410f-81bb-566071e51773",
       "rows": [
        [
         "0",
         "මොනා උනත් පොත්ත සුදු කෑල්ලක් දැකලා දෙකට නැවුනෙ නැති එකට respect කොල්ලො සමහර උන් නිකනුත් දාන් යන්න try කරනවනේ"
        ],
        [
         "1",
         "බැරිනං PickMe එකෙන් අයින් වෙලා නිකං හයර් දුවපං මේකේ ඉන්න වියළි කලාපයේ උන් සල්ලි උස්සං හැම තැනම ගියාට කොළඹ ඉන්න එවුන් හැමතැනම ගෙනියන්නේ card කියලා නොදැන මෙතල වැළලෙන්ඩ හදනවා කෙල්ලගේ හමටයි රස්සාවටයි බැන බැන පිස්සු කෙලිනවා මෙයා කවුද කියලා මම දන්නෑ නමුත් මේ ප් ‍ රශ්නේ දැන් PickMe එකේ මල වදයක් වෙලා තියෙන්නේ එක්කෝ මගදි කියනවා trip එක cancel කරන්ඩ කියලා උන්ට companyයට ගෙවන්ඩ බැරි නිසා එතකොට අපේ safety එකට කවුද වගකියන්නේ එකෙන් හයර් එක අරන් එහෙම බලු වැඩ කරන එවුන් එමටයි එකෙන් හයර් එක දුවන ගමන් card option එකට මාරු වෙන්ඩ ඉඩදීලා තියනවනං ඒකට එකඟවෙන්න riderට බැරිනං ඒක company agreement එකට විරුද්ධව යෑමක් එහෙනං අස් වෙලා හන්දි ගානේ හයර් එනකං වේලි වේලි හිටියනං හරි කවුරුත් බලෙන් PickMe එකට ගත්තේ නෑ නේ"
        ],
        [
         "2",
         "ට් ‍ රිප් එක මැදදී කෑශ් හයර් එක කාඩ් හයර් එකට චේන්ජ් කරන්න ඔප්ශන් එකක් දීලා තියනවා කියන්නේ පික් මී ඩ් ‍ රයිවර් මේ දෙකටම ඔට්ටුයි කියන එක ඕනේ වෙලාවක ඒකට මේ මල්ලි එකඟ නැතිනම් පික් මී දුවන්නැතුව හන්දියේ වීල් පාක් එකේ දුවන්න තිබ්බා අපරාදේ මේ ගෑණු ලමයගේ දවසම කෑවා"
        ],
        [
         "3",
         "me දුවපු කොල්ලෙක්ට විතරයි seen එක තේරෙන්නෙඋදේම cashඑකක් ගන්නෙ තෙල් ගහන්න අතේ සල්ලි නැතුව බන්"
        ],
        [
         "4",
         "මාත් හයර් දුවන්නෙ කස්ටමර්ගෙ පැත්තෙන් බලනකොට සාදාරනයි දෙයියෝ අද රටේ වෙන් දේවල් එක්ක මිනිස්සු විශ්වාස කරන්න බැ කෙල්ලෙක් නැග්ගම අපෙ උන් සෑහෙන්න වෙනස් වෙනවා මගදි එහෙම උනාම මිනිස්සු සල්ලි අතෙ තියගන්නෙ නැ මචං තේරුම් ගනිල්ල කාඩ් කෑශ් කියල අවුලක් නැ අපේ කැපෙන ගාන අපි අනිවාර්යයෙන්ම ගෙවන්න ඔනි තේරුනද මන් දැන් අවුරුදු 4 ක් විතර කාඩ් කෑශ් ප් ‍ රශ්නයක් නිස මුලදි මගෙ අකවුන්ට් බ්ලොක් උන නොතෙරෙන නිස ගින් සමීක්ෂනයකට ඉදල තෙරුම් අරන් අද මිනිස්සු එක්ක සාමකාමීව ලස්සනට සතුටින් මිනිසුන්ගේ සතුට දිනාගෙන ආදරෙන් වගකීමෙන් ජොබ් එක කරනවා මම මේක හරිම ලස්සන ජොබ් එකක් නිදහස් ඒ ජොබ් එක සාදාරනවා ලස්සනට කරන්න වරදින එකක් නැ කස්ටමර්ස්ල නිසයි අපි කියක් හරි හොයගන්නෙ කියන එක අමතක කරන්න එපා යාලුවා"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>මොනා උනත් පොත්ත සුදු කෑල්ලක් දැකලා දෙකට නැවුනෙ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>බැරිනං PickMe එකෙන් අයින් වෙලා නිකං හයර් දුවපං...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ට් ‍ රිප් එක මැදදී කෑශ් හයර් එක කාඩ් හයර් එකට ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>me දුවපු කොල්ලෙක්ට විතරයි seen එක තේරෙන්නෙඋදේම...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>මාත් හයර් දුවන්නෙ කස්ටමර්ගෙ පැත්තෙන් බලනකොට සා...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment\n",
       "0  මොනා උනත් පොත්ත සුදු කෑල්ලක් දැකලා දෙකට නැවුනෙ...\n",
       "1  බැරිනං PickMe එකෙන් අයින් වෙලා නිකං හයර් දුවපං...\n",
       "2  ට් ‍ රිප් එක මැදදී කෑශ් හයර් එක කාඩ් හයර් එකට ...\n",
       "3  me දුවපු කොල්ලෙක්ට විතරයි seen එක තේරෙන්නෙඋදේම...\n",
       "4  මාත් හයර් දුවන්නෙ කස්ටමර්ගෙ පැත්තෙන් බලනකොට සා..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_01 = pd.read_csv('/Volumes/KODAK/folder 02/language_translation/Language_translator/data/filtered_dataset.csv')\n",
    "\n",
    "df_01.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6d57f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning and preprocessing for Sinhala-English mixed content\n",
    "    Handles URLs, special chars, mixed language tokens, and whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    \n",
    "    # Reular expression patterns\n",
    "    sinhala_pattern = re.compile(r'[\\u0D80-\\u0DFF\\u0DE6-\\u0DEF0-9]+')\n",
    "    english_pattern = re.compile(r'[a-zA-Z]+')\n",
    "    url_pattern = re.compile(r'http\\S+|www\\S+|https\\S+', flags=re.MULTILINE)\n",
    "    special_char_pattern = re.compile(r'[[^\\u0D80-\\u0DFF\\s.,!?\\'\\\"]]')\n",
    "    special_whitespace = re.compile(r'[\\u200B-\\u200D\\uFEFF\\u00A0]')\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = url_pattern.sub('', text)\n",
    "    text = special_whitespace.sub(' ', text)\n",
    "    \n",
    "    # Split into tokens(words)\n",
    "    tokens = text.split()\n",
    "    processed_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Check langauge composition of each token\n",
    "        has_sinhala = bool(sinhala_pattern.search(token))\n",
    "        has_english = bool(english_pattern.search(token))\n",
    "        \n",
    "        # Pure Sinhala - process special chars only\n",
    "        if has_sinhala and not has_english:\n",
    "            cleaned_token = special_char_pattern.sub('', token)\n",
    "            if cleaned_token:\n",
    "                processed_tokens.append(cleaned_token)\n",
    "                \n",
    "        \n",
    "        # Mixed Sinhala-English - extract only Sinhala part\n",
    "        elif has_sinhala and has_english:\n",
    "            sinhala_parts = ''.join(sinhala_pattern.findall(token))\n",
    "            if sinhala_parts:\n",
    "                processed_tokens.append(sinhala_parts)\n",
    "                \n",
    "                \n",
    "        # Pure English - remove completely (change if you want to keep)\n",
    "        else:\n",
    "            cleaned_eng = special_char_pattern.sub('', token)\n",
    "            if cleaned_eng:\n",
    "                processed_tokens.append(f'[ENG: {cleaned_eng}]')\n",
    "                \n",
    "                \n",
    "    # Reconstruct the text\n",
    "    processed_text = ' '.join(processed_tokens)\n",
    "    \n",
    "    # Final cleaning and whitespace normalization \n",
    "    processed_text = special_char_pattern.sub('', processed_text)\n",
    "    processed_text = ' '.join(processed_text.split())\n",
    "    \n",
    "    return processed_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4542ff2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Comment\n",
      "0  මොනා උනත් පොත්ත සුදු කෑල්ලක් දැකලා දෙකට නැවුනෙ...\n",
      "1  බැරිනං PickMe එකෙන් අයින් වෙලා නිකං හයර් දුවපං...\n",
      "2  ට් ‍ රිප් එක මැදදී කෑශ් හයර් එක කාඩ් හයර් එකට ...\n",
      "3  me දුවපු කොල්ලෙක්ට විතරයි seen එක තේරෙන්නෙඋදේම...\n",
      "4  මාත් හයර් දුවන්නෙ කස්ටමර්ගෙ පැත්තෙන් බලනකොට සා...\n",
      "Total comments: 807\n",
      "Comments after cleaning: 807\n",
      "\n",
      "Original: මොනා උනත් පොත්ත සුදු කෑල්ලක් දැකලා දෙකට නැවුනෙ නැති එකට respect කොල්ලො සමහර උන් නිකනුත් දාන් යන්න try කරනවනේ\n",
      "Cleaned: මොනා උනත් පොත්ත සුදු කෑල්ලක් දැකලා දෙකට නැවුනෙ නැති එකට [ENG: respect] කොල්ලො සමහර උන් නිකනුත් දාන් යන්න [ENG: try] කරනවනේ\n",
      "\n",
      "Original: බැරිනං PickMe එකෙන් අයින් වෙලා නිකං හයර් දුවපං මේකේ ඉන්න වියළි කලාපයේ උන් සල්ලි උස්සං හැම තැනම ගියාට කොළඹ ඉන්න එවුන් හැමතැනම ගෙනියන්නේ card කියලා නොදැන මෙතල වැළලෙන්ඩ හදනවා කෙල්ලගේ හමටයි රස්සාවටයි බැන බැන පිස්සු කෙලිනවා මෙයා කවුද කියලා මම දන්නෑ නමුත් මේ ප් ‍ රශ්නේ දැන් PickMe එකේ මල වදයක් වෙලා තියෙන්නේ එක්කෝ මගදි කියනවා trip එක cancel කරන්ඩ කියලා උන්ට companyයට ගෙවන්ඩ බැරි නිසා එතකොට අපේ safety එකට කවුද වගකියන්නේ එකෙන් හයර් එක අරන් එහෙම බලු වැඩ කරන එවුන් එමටයි එකෙන් හයර් එක දුවන ගමන් card option එකට මාරු වෙන්ඩ ඉඩදීලා තියනවනං ඒකට එකඟවෙන්න riderට බැරිනං ඒක company agreement එකට විරුද්ධව යෑමක් එහෙනං අස් වෙලා හන්දි ගානේ හයර් එනකං වේලි වේලි හිටියනං හරි කවුරුත් බලෙන් PickMe එකට ගත්තේ නෑ නේ\n",
      "Cleaned: බැරිනං [ENG: PickMe] එකෙන් අයින් වෙලා නිකං හයර් දුවපං මේකේ ඉන්න වියළි කලාපයේ උන් සල්ලි උස්සං හැම තැනම ගියාට කොළඹ ඉන්න එවුන් හැමතැනම ගෙනියන්නේ [ENG: card] කියලා නොදැන මෙතල වැළලෙන්ඩ හදනවා කෙල්ලගේ හමටයි රස්සාවටයි බැන බැන පිස්සු කෙලිනවා මෙයා කවුද කියලා මම දන්නෑ නමුත් මේ ප් රශ්නේ දැන් [ENG: PickMe] එකේ මල වදයක් වෙලා තියෙන්නේ එක්කෝ මගදි කියනවා [ENG: trip] එක [ENG: cancel] කරන්ඩ කියලා උන්ට යට ගෙවන්ඩ බැරි නිසා එතකොට අපේ [ENG: safety] එකට කවුද වගකියන්නේ එකෙන් හයර් එක අරන් එහෙම බලු වැඩ කරන එවුන් එමටයි එකෙන් හයර් එක දුවන ගමන් [ENG: card] [ENG: option] එකට මාරු වෙන්ඩ ඉඩදීලා තියනවනං ඒකට එකඟවෙන්න ට බැරිනං ඒක [ENG: company] [ENG: agreement] එකට විරුද්ධව යෑමක් එහෙනං අස් වෙලා හන්දි ගානේ හයර් එනකං වේලි වේලි හිටියනං හරි කවුරුත් බලෙන් [ENG: PickMe] එකට ගත්තේ නෑ නේ\n",
      "\n",
      "Original: ට් ‍ රිප් එක මැදදී කෑශ් හයර් එක කාඩ් හයර් එකට චේන්ජ් කරන්න ඔප්ශන් එකක් දීලා තියනවා කියන්නේ පික් මී ඩ් ‍ රයිවර් මේ දෙකටම ඔට්ටුයි කියන එක ඕනේ වෙලාවක ඒකට මේ මල්ලි එකඟ නැතිනම් පික් මී දුවන්නැතුව හන්දියේ වීල් පාක් එකේ දුවන්න තිබ්බා අපරාදේ මේ ගෑණු ලමයගේ දවසම කෑවා\n",
      "Cleaned: ට් රිප් එක මැදදී කෑශ් හයර් එක කාඩ් හයර් එකට චේන්ජ් කරන්න ඔප්ශන් එකක් දීලා තියනවා කියන්නේ පික් මී ඩ් රයිවර් මේ දෙකටම ඔට්ටුයි කියන එක ඕනේ වෙලාවක ඒකට මේ මල්ලි එකඟ නැතිනම් පික් මී දුවන්නැතුව හන්දියේ වීල් පාක් එකේ දුවන්න තිබ්බා අපරාදේ මේ ගෑණු ලමයගේ දවසම කෑවා\n",
      "\n",
      "Original: me දුවපු කොල්ලෙක්ට විතරයි seen එක තේරෙන්නෙඋදේම cashඑකක් ගන්නෙ තෙල් ගහන්න අතේ සල්ලි නැතුව බන්\n",
      "Cleaned: [ENG: me] දුවපු කොල්ලෙක්ට විතරයි [ENG: seen] එක තේරෙන්නෙඋදේම එකක් ගන්නෙ තෙල් ගහන්න අතේ සල්ලි නැතුව බන්\n",
      "\n",
      "Original: මාත් හයර් දුවන්නෙ කස්ටමර්ගෙ පැත්තෙන් බලනකොට සාදාරනයි දෙයියෝ අද රටේ වෙන් දේවල් එක්ක මිනිස්සු විශ්වාස කරන්න බැ කෙල්ලෙක් නැග්ගම අපෙ උන් සෑහෙන්න වෙනස් වෙනවා මගදි එහෙම උනාම මිනිස්සු සල්ලි අතෙ තියගන්නෙ නැ මචං තේරුම් ගනිල්ල කාඩ් කෑශ් කියල අවුලක් නැ අපේ කැපෙන ගාන අපි අනිවාර්යයෙන්ම ගෙවන්න ඔනි තේරුනද මන් දැන් අවුරුදු 4 ක් විතර කාඩ් කෑශ් ප් ‍ රශ්නයක් නිස මුලදි මගෙ අකවුන්ට් බ්ලොක් උන නොතෙරෙන නිස ගින් සමීක්ෂනයකට ඉදල තෙරුම් අරන් අද මිනිස්සු එක්ක සාමකාමීව ලස්සනට සතුටින් මිනිසුන්ගේ සතුට දිනාගෙන ආදරෙන් වගකීමෙන් ජොබ් එක කරනවා මම මේක හරිම ලස්සන ජොබ් එකක් නිදහස් ඒ ජොබ් එක සාදාරනවා ලස්සනට කරන්න වරදින එකක් නැ කස්ටමර්ස්ල නිසයි අපි කියක් හරි හොයගන්නෙ කියන එක අමතක කරන්න එපා යාලුවා\n",
      "Cleaned: මාත් හයර් දුවන්නෙ කස්ටමර්ගෙ පැත්තෙන් බලනකොට සාදාරනයි දෙයියෝ අද රටේ වෙන් දේවල් එක්ක මිනිස්සු විශ්වාස කරන්න බැ කෙල්ලෙක් නැග්ගම අපෙ උන් සෑහෙන්න වෙනස් වෙනවා මගදි එහෙම උනාම මිනිස්සු සල්ලි අතෙ තියගන්නෙ නැ මචං තේරුම් ගනිල්ල කාඩ් කෑශ් කියල අවුලක් නැ අපේ කැපෙන ගාන අපි අනිවාර්යයෙන්ම ගෙවන්න ඔනි තේරුනද මන් දැන් අවුරුදු 4 ක් විතර කාඩ් කෑශ් ප් රශ්නයක් නිස මුලදි මගෙ අකවුන්ට් බ්ලොක් උන නොතෙරෙන නිස ගින් සමීක්ෂනයකට ඉදල තෙරුම් අරන් අද මිනිස්සු එක්ක සාමකාමීව ලස්සනට සතුටින් මිනිසුන්ගේ සතුට දිනාගෙන ආදරෙන් වගකීමෙන් ජොබ් එක කරනවා මම මේක හරිම ලස්සන ජොබ් එකක් නිදහස් ඒ ජොබ් එක සාදාරනවා ලස්සනට කරන්න වරදින එකක් නැ කස්ටමර්ස්ල නිසයි අපි කියක් හරි හොයගන්නෙ කියන එක අමතක කරන්න එපා යාලුවා\n"
     ]
    }
   ],
   "source": [
    "# load the pipeline\n",
    "\n",
    "df = pd.read_csv('/Volumes/KODAK/folder 02/language_translation/Language_translator/data/filtered_dataset.csv')\n",
    "\n",
    "# check the structure\n",
    "print(df.head())\n",
    "print(f'Total comments: {len(df)}')\n",
    "\n",
    "# Apply the combined cleaning function\n",
    "df['cleaned_comment'] = df['Comment'].apply(clean_and_preprocess_text)\n",
    "\n",
    "# Remove empty comments after cleaning\n",
    "df = df[df['cleaned_comment'].str.len() > 0]\n",
    "print(f'Comments after cleaning: {len(df)}')\n",
    "\n",
    "\n",
    "# Show some sample\n",
    "for idx, row in df.head(5).iterrows():\n",
    "    print(f'\\nOriginal: {row['Comment']}')\n",
    "    print(f'Cleaned: {row['cleaned_comment']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aa00510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_helsinki(text):\n",
    "    try:\n",
    "        if not text or len(text) == 0:\n",
    "            return \"\"\n",
    "        # Initialize model (will cache after first load)\n",
    "        translator = pipeline(\n",
    "            \"translation\", \n",
    "            model=\"Helsinki-NLP/opus-mt-si-en\",\n",
    "            device=0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "        result = translator(text, max_length=400)[0]['translation_text']\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error translating with Helsinki: {str(e)}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ec8c895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_google(text):\n",
    "    try:\n",
    "        if not text or len(text) == 0:\n",
    "            return \"\"\n",
    "        translator = Translator()\n",
    "        # Random delay to avoid rate limiting\n",
    "        time.sleep(random.uniform(0.5, 1.5))\n",
    "        result = translator.translate(text, src='si', dest='en').text\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error translating with Google: {str(e)}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41c1883",
   "metadata": {},
   "source": [
    "# Hybrid Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c3a15c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_hybrid(text, max_retries = 3):\n",
    "    if not text or len(text) == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    # Try Helsinki first\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = translate_helsinki(text)\n",
    "            if result and len(result) > 0:\n",
    "                return result\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            \n",
    "            \n",
    "    # Fallback to Google Translate\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = translate_google(text)\n",
    "            if result and len(result) > 0:\n",
    "                return result\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            \n",
    "    return \"\" # Return empty string if all attempts fail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3405dc1",
   "metadata": {},
   "source": [
    "# Parallel Translation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "78a7185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_translate(df_chunk):\n",
    "    return df_chunk['cleaned_comment'].progress_apply(translate_hybrid)\n",
    "\n",
    "def translate_in_parallel(df, n_workers = None):\n",
    "    if n_workers is None:\n",
    "        n_workers = max(1, cpu_count() - 1)\n",
    "        \n",
    "        \n",
    "    df_split = np.array_split(df, n_workers)\n",
    "    \n",
    "    with Pool(n_workers) as pool:\n",
    "        results = pool.map(parallel_translate, df_split)\n",
    "        \n",
    "    # Combine results\n",
    "    return pd.concat(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a4172e",
   "metadata": {},
   "source": [
    "# Execute Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "51766f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['Comment']\n",
      "Starting translation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "Process SpawnPoolWorker-52:\n",
      "Process SpawnPoolWorker-56:\n",
      "Process SpawnPoolWorker-54:\n",
      "Process SpawnPoolWorker-50:\n",
      "Process SpawnPoolWorker-53:\n",
      "Process SpawnPoolWorker-55:\n",
      "Process SpawnPoolWorker-51:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 389, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_chunk' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Process SpawnPoolWorker-60:\n",
      "Process SpawnPoolWorker-62:\n",
      "Process SpawnPoolWorker-61:\n",
      "Process SpawnPoolWorker-63:\n",
      "Process SpawnPoolWorker-59:\n",
      "Process SpawnPoolWorker-58:\n",
      "Process SpawnPoolWorker-57:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 106\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting translation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m     sample_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslated_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m parallel_translate(sample_df)\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# Show results\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m sample_df\u001b[38;5;241m.\u001b[39mhead()\u001b[38;5;241m.\u001b[39miterrows():\n",
      "Cell \u001b[0;32mIn[41], line 88\u001b[0m, in \u001b[0;36mparallel_translate\u001b[0;34m(df, column, n_workers)\u001b[0m\n\u001b[1;32m     85\u001b[0m func \u001b[38;5;241m=\u001b[39m partial(process_chunk, func\u001b[38;5;241m=\u001b[39mtranslate_hybrid)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(n_workers) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 88\u001b[0m     results \u001b[38;5;241m=\u001b[39m pool\u001b[38;5;241m.\u001b[39mmap(func, [chunk[column] \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m df_split])\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(results)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_async(func, iterable, mapstar, chunksize)\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event\u001b[38;5;241m.\u001b[39mwait(timeout)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import random\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from googletrans import Translator\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Translation functions\n",
    "def translate_helsinki(text):\n",
    "    try:\n",
    "        if not text or len(text) == 0:\n",
    "            return \"\"\n",
    "        translator = pipeline(\n",
    "            \"translation\", \n",
    "            model=\"Helsinki-NLP/opus-mt-si-en\",\n",
    "            device=0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "        result = translator(text, max_length=400)[0]['translation_text']\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Helsinki error: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def translate_google(text):\n",
    "    try:\n",
    "        if not text or len(text) == 0:\n",
    "            return \"\"\n",
    "        translator = Translator()\n",
    "        time.sleep(random.uniform(0.5, 1.5))\n",
    "        result = translator.translate(text, src='si', dest='en').text\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Google error: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def translate_hybrid(text, max_retries=3):\n",
    "    if not text or len(text) == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = translate_helsinki(text)\n",
    "            if result: return result\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = translate_google(text)\n",
    "            if result: return result\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "# Parallel processing\n",
    "def process_chunk(chunk, func):\n",
    "    return chunk.apply(func)\n",
    "\n",
    "def parallel_translate(df, column=None, n_workers=None):\n",
    "    \"\"\"Auto-detects text column if not specified\"\"\"\n",
    "    if column is None:\n",
    "        possible_cols = ['cleaned_comment', 'comment', 'text', 'content', 'sentence']\n",
    "        for col in possible_cols + df.columns.tolist():\n",
    "            if col in df.columns:\n",
    "                column = col\n",
    "                break\n",
    "    \n",
    "    if column not in df.columns:\n",
    "        raise KeyError(f\"Column '{column}' not found in DataFrame. Available columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    if n_workers is None:\n",
    "        n_workers = max(1, cpu_count() - 1)\n",
    "    \n",
    "    df_split = np.array_split(df, n_workers)\n",
    "    func = partial(process_chunk, func=translate_hybrid)\n",
    "    \n",
    "    with Pool(n_workers) as pool:\n",
    "        results = pool.map(func, [chunk[column] for chunk in df_split])\n",
    "    \n",
    "    return pd.concat(results)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == '__main__':\n",
    "    # Load data\n",
    "    df = pd.read_csv('/Volumes/KODAK/folder 02/language_translation/Language_translator/data/filtered_dataset.csv')\n",
    "    \n",
    "    # Check columns\n",
    "    print(\"Available columns:\", df.columns.tolist())\n",
    "    \n",
    "    # Sample data\n",
    "    sample_df = df.sample(10, random_state=42) if len(df) > 100 else df.copy()\n",
    "    \n",
    "    # Translate\n",
    "    print(\"Starting translation...\")\n",
    "    try:\n",
    "        sample_df['translated_text'] = parallel_translate(sample_df)\n",
    "        \n",
    "        # Show results\n",
    "        for idx, row in sample_df.head().iterrows():\n",
    "            print(f\"\\nOriginal: {row.get('cleaned_comment', row.iloc[0])}\")  # Gets first column if 'cleaned_comment' doesn't exist\n",
    "            print(f\"Translated: {row['translated_text']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during translation: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcf3605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['Comment']\n",
      "Starting translation...\n",
      "Using text column: Comment\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af45fd841bd452db0358eb420676d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Helsinki error: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original: days all the pickme drivers are bad\n",
      "Translated: days all the pickme drivers are bad\n",
      "\n",
      "Original: hayar yanna ai baya mekai siddiya me hayar eka yaddi kalin giya hayar valata ganak PickMe ekata gewanna thiyanawa meka giyoth adu wela tmi labenne godak kattiya hayar ganne nethhe okai habai hamoma mehema ne e nisa okkotama dos kiyana eka weradi meka e riyadurage weraddak\n",
      "Translated: Hayar Yanna Ai Siddiya ME Hayar Eka kalin giya giya giyawa gika gika ganna giyanawa genna ganna goakaHamoma Mehema NEI OKKOTAMA DOS KIYANA EKA WERADI MEKA E RIYADURAGE WERADDAK\n",
      "\n",
      "Original: හයර් එක එයා approve කරන්න ඇත්තෙ cash ද card ද කියල බලල මෙයා මගදි තමයි card payment එකකට මාරු වෙලා තියෙන්නෙ ඒක අසාධාරණයි මෙයා කලින්ම card payment කියල තිබුනනම් මේ හයර් එක ත් ‍ රීවීල් එකේ කෙනා ගන්නෙ නෑ eken ගෙව්වවම ඒක යන්නෙ pick me එකට ඒ සල්ලි එයාට ලැබෙන්නෙ පස්සෙඅවුරුදු 6 ක් pick me app එකෙන් ගියානම් දන්නෙ නැද්ද කලින්ම cash ද card ද කියල දාන්න\n",
      "Translated: Here is the one who appses the Cash and Card, who is moving to a card that is unforded to get that money in the pickeeel, if he gets that money in the picke.Don't know whether Cash is also a CARD\n",
      "\n",
      "Original: පියුමි නෙවි කාට උනත් මේක අසාදාරනයි මට හැමදාම මන් දන්න අයට හැමදාම වෙන දේපුදුම මලක් පනින්නේ මේ පික් මී අයට එයාලට company එකෙන් ගෙවන ගාන අඩුයි නම් කාඩ් වලින් පේ කරනවා නම් ලොකේශන් එකට එන්න කලින් එක්කෝ කියන්න ඕන අපිත් වාහනේ එනකන් බලන් ඉදලා ආවට පස්සේ තව රස්තියාදු කරලා කොච්චර අපිව පාරවල් වල බස්සල ගිහින් ඇත්ද මම කොච්චර අසරන වෙලා ඇද්ද උදේම ගමනක් ‍ යද්දි atm එකක් හොයන් යන්න ගියාමප් ‍ රශ්නේ වෙන කෙනෙක් තමයි ඒක දන්නේ\n",
      "Translated: No matter how many people are uneducated, I can either say how much I have been buried in the road if they were waiting for the company, when they were waiting for the company.It knows that a ATM is going to leave the atm when a journey\n",
      "\n",
      "Original: එකේ තියෙන්නෙත් මගෙ සල්ලි බන් වෙන උනගෙ නෙමේ අනික උබලට දවසක් ඇතුලත සල්ලි ලැබෙනවනෙ මන් uber දැනට ගියපු හැම හයර් එකක්ම card uber car වල drivers ල genuine pick me use karn naththe ekai\n",
      "Translated: One of the other tubers that my money was not paid for one day, the driine pick me use every one who had gone in the uber uber card charn naththe ekai\n",
      "\n",
      "Original: log a complaint to pickme\n",
      "Translated: Log A COMPLAINT TO PICKME\n",
      "\n",
      "Original: monatethe card dhanna miss cash dhemmanam ivarai ne card dhemmoth salli kattiya hatheta ennenehe pick me company ekata gihilla mainass vela podi gahana thamai enne eka recommended karanna puluvang\n",
      "Translated: moonatethe card dhemmamanam podihe podi podi gihilla pulai gihilla pulana pulranna puluvang\n",
      "\n",
      "Original: වාත කේස් එකක් කොහේ ගියත් හානවනේ අම්මපා\n",
      "Translated: The aircra case goes where the motherpah\n",
      "\n",
      "Original: මෙන්න මෙවුවට තමා මයෙ අක්කේ complication කියලා කියන්නේ\n",
      "Translated: Here is her Sister Complication\n",
      "\n",
      "Original: geniyek kisi oluwak ne gon kama penwanna epa poththa sudu una kiyala cash denmanam cash gewanna passe card gena kiyawanne nethuwa weredda thamungemai moda kama panwanna epa\n",
      "Translated: geniyek kisi oluwna epa poththa dudu kiwanna manmanna manmanna manmanna manmanna ohdda thamunne gena kama kama kama kama panwanna EPA\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from googletrans import Translator\n",
    "import logging\n",
    "import concurrent.futures\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define translation functions\n",
    "def translate_helsinki(text):\n",
    "    try:\n",
    "        if not text or len(text) == 0:\n",
    "            return \"\"\n",
    "        # Initialize model (will cache after first load)\n",
    "        translator = pipeline(\n",
    "            \"translation\", \n",
    "            model=\"Helsinki-NLP/opus-mt-si-en\",\n",
    "            device=0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "        result = translator(text, max_length=400)[0]['translation_text']\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Helsinki error: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def translate_google(text):\n",
    "    try:\n",
    "        if not text or len(text) == 0:\n",
    "            return \"\"\n",
    "        translator = Translator()\n",
    "        time.sleep(random.uniform(0.5, 1.5))\n",
    "        result = translator.translate(text, src='si', dest='en').text\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Google error: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def translate_hybrid(text, max_retries=3):\n",
    "    if not text or len(text) == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    # Try Helsinki first\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = translate_helsinki(text)\n",
    "            if result and len(result) > 0:\n",
    "                return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Helsinki attempt {attempt+1} failed: {str(e)}\")\n",
    "            time.sleep(1)\n",
    "    \n",
    "    # Fallback to Google\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = translate_google(text)\n",
    "            if result and len(result) > 0:\n",
    "                return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Google attempt {attempt+1} failed: {str(e)}\")\n",
    "            time.sleep(1)\n",
    "    \n",
    "    return \"\"  # Return empty if all attempts fail\n",
    "\n",
    "# Avoid multiprocessing in Jupyter notebooks - use sequential processing instead\n",
    "def translate_all(texts):\n",
    "    \"\"\"Translate a list of texts sequentially with progress bar\"\"\"\n",
    "    results = []\n",
    "    for text in tqdm(texts, desc=\"Translating\"):\n",
    "        results.append(translate_hybrid(text))\n",
    "    return results\n",
    "\n",
    "# Find the appropriate text column in the dataframe\n",
    "def get_text_column(df):\n",
    "    possible_cols = ['cleaned_comment', 'comment', 'text', 'content', 'sentence', 'Comment']\n",
    "    for col in possible_cols:\n",
    "        if col in df.columns:\n",
    "            return col\n",
    "    # Default to first column if none of the expected columns are found\n",
    "    return df.columns[0] if len(df.columns) > 0 else None\n",
    "\n",
    "# Version using ThreadPoolExecutor which works better in Jupyter notebooks\n",
    "def parallel_translate_threaded(df, column=None, n_workers=None):\n",
    "    \"\"\"Translate texts using ThreadPoolExecutor which works better in Jupyter\"\"\"\n",
    "    if column is None:\n",
    "        column = get_text_column(df)\n",
    "    \n",
    "    if column not in df.columns:\n",
    "        raise KeyError(f\"Column '{column}' not found in DataFrame. Available columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    if n_workers is None:\n",
    "        import os\n",
    "        n_workers = max(1, os.cpu_count() - 1)\n",
    "    \n",
    "    # Convert the column to a list\n",
    "    texts = df[column].tolist()\n",
    "    results = []\n",
    "    \n",
    "    # Use ThreadPoolExecutor instead of ProcessPoolExecutor to avoid pickling issues\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        futures = [executor.submit(translate_hybrid, text) for text in texts]\n",
    "        \n",
    "        # Process results as they complete with a progress bar\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Translating\"):\n",
    "            results.append(future.result())\n",
    "    \n",
    "    # Reorder results to match input order\n",
    "    ordered_results = [None] * len(texts)\n",
    "    for i, future in enumerate(futures):\n",
    "        try:\n",
    "            ordered_results[i] = future.result()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in thread {i}: {str(e)}\")\n",
    "            ordered_results[i] = \"\"\n",
    "    \n",
    "    return ordered_results\n",
    "\n",
    "# Main execution in Jupyter-friendly way\n",
    "# Load data\n",
    "try:\n",
    "    file_path = '/Volumes/KODAK/folder 02/language_translation/Language_translator/data/filtered_dataset.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Check columns\n",
    "    print(\"Available columns:\", df.columns.tolist())\n",
    "    \n",
    "    # Sample data for testing\n",
    "    sample_size = min(10, len(df))\n",
    "    sample_df = df.sample(sample_size, random_state=42)\n",
    "    \n",
    "    print(\"Starting translation...\")\n",
    "    \n",
    "    # Identify the text column\n",
    "    text_column = get_text_column(sample_df)\n",
    "    print(f\"Using text column: {text_column}\")\n",
    "    \n",
    "    # Translate using ThreadPoolExecutor approach\n",
    "    translated_texts = parallel_translate_threaded(sample_df, column=text_column)\n",
    "    \n",
    "    # Add translations to the dataframe\n",
    "    sample_df['translated_text'] = translated_texts\n",
    "    \n",
    "    # Show results\n",
    "    for idx, row in sample_df.iterrows():\n",
    "        print(f\"\\nOriginal: {row[text_column]}\")\n",
    "        print(f\"Translated: {row['translated_text']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during execution: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a8bcc402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['Comment']\n",
      "Starting translation with focus on quality...\n",
      "Using text column: Comment\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273f049e4ef243b2be9c18b8c5be1833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading Helsinki-NLP model...\n",
      "ERROR:__main__:Error loading Helsinki model: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Error in fallback Helsinki initialization: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Failed to initialize Helsinki translator\n",
      "INFO:__main__:Loading Helsinki-NLP model...\n",
      "ERROR:__main__:Error loading Helsinki model: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Error in fallback Helsinki initialization: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Failed to initialize Helsinki translator\n",
      "INFO:__main__:Loading Helsinki-NLP model...\n",
      "ERROR:__main__:Error loading Helsinki model: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Error in fallback Helsinki initialization: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Failed to initialize Helsinki translator\n",
      "INFO:__main__:Loading Helsinki-NLP model...\n",
      "ERROR:__main__:Error loading Helsinki model: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Error in fallback Helsinki initialization: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Failed to initialize Helsinki translator\n",
      "INFO:__main__:Loading Helsinki-NLP model...\n",
      "ERROR:__main__:Error loading Helsinki model: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Error in fallback Helsinki initialization: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Failed to initialize Helsinki translator\n",
      "INFO:__main__:Loading Helsinki-NLP model...\n",
      "ERROR:__main__:Error loading Helsinki model: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Error in fallback Helsinki initialization: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Failed to initialize Helsinki translator\n",
      "INFO:__main__:Loading Helsinki-NLP model...\n",
      "ERROR:__main__:Error loading Helsinki model: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Error in fallback Helsinki initialization: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Failed to initialize Helsinki translator\n",
      "INFO:__main__:Loading Helsinki-NLP model...\n",
      "ERROR:__main__:Error loading Helsinki model: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Error in fallback Helsinki initialization: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Failed to initialize Helsinki translator\n",
      "INFO:__main__:Loading Helsinki-NLP model...\n",
      "ERROR:__main__:Error loading Helsinki model: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Error in fallback Helsinki initialization: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Failed to initialize Helsinki translator\n",
      "INFO:__main__:Loading Helsinki-NLP model...\n",
      "ERROR:__main__:Error loading Helsinki model: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Error in fallback Helsinki initialization: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Failed to initialize Helsinki translator\n",
      "INFO:__main__:Loading Helsinki-NLP model...\n",
      "ERROR:__main__:Error loading Helsinki model: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Error in fallback Helsinki initialization: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Failed to initialize Helsinki translator\n",
      "INFO:__main__:Loading Helsinki-NLP model...\n",
      "ERROR:__main__:Error loading Helsinki model: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Error in fallback Helsinki initialization: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Failed to initialize Helsinki translator\n",
      "INFO:__main__:Loading Helsinki-NLP model...\n",
      "ERROR:__main__:Error loading Helsinki model: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Error in fallback Helsinki initialization: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Failed to initialize Helsinki translator\n",
      "INFO:__main__:Loading Helsinki-NLP model...\n",
      "ERROR:__main__:Error loading Helsinki model: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Error in fallback Helsinki initialization: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Failed to initialize Helsinki translator\n",
      "INFO:__main__:Loading Helsinki-NLP model...\n",
      "ERROR:__main__:Error loading Helsinki model: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Error in fallback Helsinki initialization: Helsinki-NLP/opus-mt-si-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "ERROR:__main__:Failed to initialize Helsinki translator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original: days all the pickme drivers are bad\n",
      "Translated: Days all the pickme drivers are bad\n",
      "\n",
      "Original: hayar yanna ai baya mekai siddiya me hayar eka yaddi kalin giya hayar valata ganak PickMe ekata gewanna thiyanawa meka giyoth adu wela tmi labenne godak kattiya hayar ganne nethhe okai habai hamoma mehema ne e nisa okkotama dos kiyana eka weradi meka e riyadurage weraddak\n",
      "Translated: Hayar yanna ai siddiya me hayar eka kalin kalin giya giya gyaar ganak pickme ekata gewanna tiyanawa meka deiyoh a adu wella tmi labenne godne nethhe okai habai habai hamoma mehema nei okkotama dos kiyana eka weradi meka e riyadurage weraddak\n",
      "\n",
      "Original: හයර් එක එයා approve කරන්න ඇත්තෙ cash ද card ද කියල බලල මෙයා මගදි තමයි card payment එකකට මාරු වෙලා තියෙන්නෙ ඒක අසාධාරණයි මෙයා කලින්ම card payment කියල තිබුනනම් මේ හයර් එක ත් ‍ රීවීල් එකේ කෙනා ගන්නෙ නෑ eken ගෙව්වවම ඒක යන්නෙ pick me එකට ඒ සල්ලි එයාට ලැබෙන්නෙ පස්සෙඅවුරුදු 6 ක් pick me app එකෙන් ගියානම් දන්නෙ නැද්ද කලින්ම cash ද card ද කියල දාන්න\n",
      "Translated: Here is a new one that is ahead, he is moving to a card payment there is unreasonable, if he had been unfair, this hair would not take the heavel when eken is paying, it'll get that money to the pick me, if he gets the pic m don't know whether cash is also a card\n",
      "\n",
      "Original: පියුමි නෙවි කාට උනත් මේක අසාදාරනයි මට හැමදාම මන් දන්න අයට හැමදාම වෙන දේපුදුම මලක් පනින්නේ මේ පික් මී අයට එයාලට company එකෙන් ගෙවන ගාන අඩුයි නම් කාඩ් වලින් පේ කරනවා නම් ලොකේශන් එකට එන්න කලින් එක්කෝ කියන්න ඕන අපිත් වාහනේ එනකන් බලන් ඉදලා ආවට පස්සේ තව රස්තියාදු කරලා කොච්චර අපිව පාරවල් වල බස්සල ගිහින් ඇත්ද මම කොච්චර අසරන වෙලා ඇද්ද උදේම ගමනක් ‍ යද්දි atm එකක් හොයන් යන්න ගියාමප් ‍ රශ්නේ වෙන කෙනෙක් තමයි ඒක දන්නේ\n",
      "Translated: No matter how not a lot of piyumi, it is uneducated and i will always jump every other thing to those who know if they are low by the computer, they either come together before they come together to the bosses tell us to say that when we waited for some of the car, we go down on the roads and how much he faded it's how much i have been in the morning when i have been in the morning you go to a atm and someone else goes to let go knowing\n",
      "\n",
      "Original: එකේ තියෙන්නෙත් මගෙ සල්ලි බන් වෙන උනගෙ නෙමේ අනික උබලට දවසක් ඇතුලත සල්ලි ලැබෙනවනෙ මන් uber දැනට ගියපු හැම හයර් එකක්ම card uber car වල drivers ල genuine pick me use karn naththe ekai\n",
      "Translated: It's not my money's not my money, but i receive the money within one day i got the money in the uber. Every herrirs drives genuine pick me use karn nathe ekai\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from googletrans import Translator\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Global variables to avoid reloading models\n",
    "helsinki_translator = None\n",
    "google_translator = None\n",
    "\n",
    "# -------------------- Text Processing Functions --------------------\n",
    "\n",
    "def clean_sinhala_text(text):\n",
    "    \"\"\"Special preprocessing for Sinhala text\"\"\"\n",
    "    if not text or len(text) == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Handle mixed English-Sinhala text\n",
    "    # Extract English words/terms that should be preserved\n",
    "    english_terms = re.findall(r'[a-zA-Z]+', text)\n",
    "    \n",
    "    # Replace numbers and special characters with spaces\n",
    "    text = re.sub(r'[^\\u0D80-\\u0DFFa-zA-Z\\s.,!?]', ' ', text)\n",
    "    \n",
    "    # Clean up extra spaces again\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def post_process_translation(text):\n",
    "    \"\"\"Clean up translation results\"\"\"\n",
    "    if not text or len(text) == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    # Fix common translation artifacts\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r' \\.', '.', text)\n",
    "    text = re.sub(r' ,', ',', text)\n",
    "    \n",
    "    # Remove repetitive phrases (common in poor translations)\n",
    "    sentences = text.split('. ')\n",
    "    unique_sentences = []\n",
    "    for s in sentences:\n",
    "        if s and s not in unique_sentences:\n",
    "            unique_sentences.append(s)\n",
    "    \n",
    "    # Rejoin and capitalize\n",
    "    text = '. '.join(s.capitalize() for s in unique_sentences if s)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def chunk_text_advanced(text, max_length=100):\n",
    "    \"\"\"Break text into smaller chunks with better boundaries\"\"\"\n",
    "    if not text or len(text) <= max_length:\n",
    "        return [text]\n",
    "    \n",
    "    # Try to split on punctuation first\n",
    "    chunks = []\n",
    "    pattern = r'([።,.!?])'\n",
    "    sentences = re.split(pattern, text)\n",
    "    \n",
    "    current_chunk = \"\"\n",
    "    for i in range(0, len(sentences), 2):\n",
    "        sentence = sentences[i]\n",
    "        punctuation = sentences[i+1] if i+1 < len(sentences) else \"\"\n",
    "        \n",
    "        if len(current_chunk) + len(sentence) + len(punctuation) <= max_length:\n",
    "            current_chunk += sentence + punctuation\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            current_chunk = sentence + punctuation\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    # If no good sentence boundaries, try word boundaries\n",
    "    if not chunks or len(chunks) == 1 and len(chunks[0]) > max_length:\n",
    "        chunks = []\n",
    "        words = text.split()\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for word in words:\n",
    "            if len(current_chunk) + len(word) + 1 <= max_length:\n",
    "                current_chunk += \" \" + word if current_chunk else word\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk)\n",
    "                current_chunk = word\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "    \n",
    "    # Last resort: character-based chunking\n",
    "    if not chunks:\n",
    "        chunks = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# -------------------- Translation Functions --------------------\n",
    "\n",
    "def get_helsinki_translator():\n",
    "    \"\"\"Initialize Helsinki translator only once\"\"\"\n",
    "    global helsinki_translator\n",
    "    if helsinki_translator is None:\n",
    "        try:\n",
    "            logger.info(\"Loading Helsinki-NLP model...\")\n",
    "            # Use specific tokenizer and model to ensure quality\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-si-en\")\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-si-en\")\n",
    "            device = 0 if torch.cuda.is_available() else -1\n",
    "            \n",
    "            helsinki_translator = pipeline(\n",
    "                \"translation\", \n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                device=device\n",
    "            )\n",
    "            logger.info(\"Helsinki-NLP model loaded successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading Helsinki model: {str(e)}\")\n",
    "            # Fall back to simpler initialization if the above fails\n",
    "            try:\n",
    "                helsinki_translator = pipeline(\n",
    "                    \"translation\", \n",
    "                    model=\"Helsinki-NLP/opus-mt-si-en\",\n",
    "                    device=0 if torch.cuda.is_available() else -1\n",
    "                )\n",
    "            except Exception as e2:\n",
    "                logger.error(f\"Error in fallback Helsinki initialization: {str(e2)}\")\n",
    "                return None\n",
    "    \n",
    "    return helsinki_translator\n",
    "\n",
    "def translate_helsinki_improved(text):\n",
    "    \"\"\"Enhanced Helsinki translation with better preprocessing\"\"\"\n",
    "    try:\n",
    "        if not text or len(text) == 0:\n",
    "            return \"\"\n",
    "        \n",
    "        # Clean text specifically for Sinhala\n",
    "        text = clean_sinhala_text(text)\n",
    "        \n",
    "        # Get or initialize translator\n",
    "        translator = get_helsinki_translator()\n",
    "        if not translator:\n",
    "            logger.error(\"Failed to initialize Helsinki translator\")\n",
    "            return \"\"\n",
    "        \n",
    "        # Break into smaller chunks for better quality\n",
    "        chunks = chunk_text_advanced(text, max_length=80)  # Smaller chunks for better quality\n",
    "        translated_chunks = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            try:\n",
    "                # Use temperature for slightly more fluent translations\n",
    "                result = translator(chunk, max_length=512, do_sample=True, temperature=0.7)[0]['translation_text']\n",
    "                translated_chunks.append(result)\n",
    "                # Small delay to avoid resource issues\n",
    "                time.sleep(0.2)\n",
    "            except Exception as chunk_error:\n",
    "                logger.error(f\"Error translating chunk: {str(chunk_error)}\")\n",
    "                # Try again with a simpler approach\n",
    "                try:\n",
    "                    result = translator(chunk, max_length=400)[0]['translation_text']\n",
    "                    translated_chunks.append(result)\n",
    "                except:\n",
    "                    # If all else fails, append an empty string to maintain chunk alignment\n",
    "                    translated_chunks.append(\"\")\n",
    "        \n",
    "        # Join and post-process\n",
    "        full_translation = ' '.join(chunk for chunk in translated_chunks if chunk)\n",
    "        return post_process_translation(full_translation)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Helsinki improved translation error: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def get_google_translator():\n",
    "    \"\"\"Initialize Google translator only once\"\"\"\n",
    "    global google_translator\n",
    "    if google_translator is None:\n",
    "        try:\n",
    "            google_translator = Translator()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing Google translator: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    return google_translator\n",
    "\n",
    "def translate_google_improved(text):\n",
    "    \"\"\"Enhanced Google translation with better preprocessing\"\"\"\n",
    "    try:\n",
    "        if not text or len(text) == 0:\n",
    "            return \"\"\n",
    "        \n",
    "        # Clean text specifically for Sinhala\n",
    "        text = clean_sinhala_text(text)\n",
    "        \n",
    "        # Get or initialize translator\n",
    "        translator = get_google_translator()\n",
    "        if not translator:\n",
    "            logger.error(\"Failed to initialize Google translator\")\n",
    "            return \"\"\n",
    "        \n",
    "        # Break into smaller chunks for better quality\n",
    "        chunks = chunk_text_advanced(text, max_length=100)\n",
    "        translated_chunks = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            try:\n",
    "                # Random delay to avoid rate limiting\n",
    "                time.sleep(random.uniform(0.7, 2.0))\n",
    "                result = translator.translate(chunk, src='si', dest='en').text\n",
    "                translated_chunks.append(result)\n",
    "            except Exception as chunk_error:\n",
    "                logger.error(f\"Error translating chunk with Google: {str(chunk_error)}\")\n",
    "                # Try again after a longer delay\n",
    "                try:\n",
    "                    time.sleep(3.0)\n",
    "                    result = translator.translate(chunk, src='si', dest='en').text\n",
    "                    translated_chunks.append(result)\n",
    "                except:\n",
    "                    # If all else fails, append an empty string to maintain chunk alignment\n",
    "                    translated_chunks.append(\"\")\n",
    "        \n",
    "        # Join and post-process\n",
    "        full_translation = ' '.join(chunk for chunk in translated_chunks if chunk)\n",
    "        return post_process_translation(full_translation)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Google improved translation error: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def translate_improved(text, max_retries=3):\n",
    "    \"\"\"Try multiple translation approaches with better quality settings\"\"\"\n",
    "    if not text or len(text) == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    # Keep track of all successful translations\n",
    "    translations = []\n",
    "    \n",
    "    # Try Helsinki with improved settings\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = translate_helsinki_improved(text)\n",
    "            if result and len(result) > len(text)/4:  # Ensure meaningful translation\n",
    "                translations.append(result)\n",
    "                break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Helsinki attempt {attempt+1} failed: {str(e)}\")\n",
    "            time.sleep(1.5)\n",
    "    \n",
    "    # Try Google with improved settings\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = translate_google_improved(text)\n",
    "            if result and len(result) > len(text)/4:\n",
    "                translations.append(result)\n",
    "                break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Google attempt {attempt+1} failed: {str(e)}\")\n",
    "            time.sleep(2)\n",
    "    \n",
    "    # If at least one translation succeeded\n",
    "    if translations:\n",
    "        # Choose based on quality heuristics (currently using length as a proxy for completeness)\n",
    "        # This could be improved with more sophisticated metrics\n",
    "        translations.sort(key=len, reverse=True)\n",
    "        return translations[0]\n",
    "    \n",
    "    # Last resort: try direct Helsinki translation without chunking\n",
    "    try:\n",
    "        translator = get_helsinki_translator()\n",
    "        if translator:\n",
    "            result = translator(clean_sinhala_text(text), max_length=512)[0]['translation_text']\n",
    "            return post_process_translation(result)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return \"\"  # Return empty if all attempts fail\n",
    "\n",
    "# -------------------- Main Processing Function --------------------\n",
    "\n",
    "def translate_all_sequential(texts):\n",
    "    \"\"\"Process all texts sequentially for maximum reliability\"\"\"\n",
    "    results = []\n",
    "    for text in tqdm(texts, desc=\"Translating\"):\n",
    "        results.append(translate_improved(text))\n",
    "    return results\n",
    "\n",
    "# -------------------- Main Execution --------------------\n",
    "\n",
    "# Main execution section - for Jupyter notebook use\n",
    "try:\n",
    "    # Load data\n",
    "    file_path = '/Volumes/KODAK/folder 02/language_translation/Language_translator/data/filtered_dataset.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Check columns\n",
    "    print(\"Available columns:\", df.columns.tolist())\n",
    "    \n",
    "    # Sample data for testing - smaller sample to focus on quality\n",
    "    sample_size = min(5, len(df))\n",
    "    sample_df = df.sample(sample_size, random_state=42)\n",
    "    \n",
    "    print(\"Starting translation with focus on quality...\")\n",
    "    \n",
    "    # Identify the text column\n",
    "    text_column = 'Comment' if 'Comment' in df.columns else df.columns[0]\n",
    "    print(f\"Using text column: {text_column}\")\n",
    "    \n",
    "    # Use sequential processing for maximum reliability\n",
    "    translated_texts = translate_all_sequential(sample_df[text_column].tolist())\n",
    "    \n",
    "    # Add translations to the dataframe\n",
    "    sample_df['translated_text'] = translated_texts\n",
    "    \n",
    "    # Show results\n",
    "    for idx, row in sample_df.iterrows():\n",
    "        print(f\"\\nOriginal: {row[text_column]}\")\n",
    "        print(f\"Translated: {row['translated_text']}\")\n",
    "    \n",
    "    # Optional: Save results to CSV\n",
    "    # sample_df.to_csv('/Volumes/KODAK/folder 02/language_translation/Language_translator/data/translated_sample_improved.csv', index=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during execution: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4987ee0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['Comment']\n",
      "Starting translation with focus on quality...\n",
      "Using text column: Comment\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f1625eda65449db5327cc744e3dc3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading Helsinki-NLP model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36d6f88a4e6447f8dd527892b469a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1577b8127484cc8bc4fc3cb8f0e622f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b02f9d82dc0477d8ab92085e696c335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/707k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3853087d0624c30b8c3e0c85c4400f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/791k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4575970e1c28490a95ff236149bc205a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884c865fba954c7f8cce3c5352a343f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/310M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/Helsinki-NLP/opus-mt-mul-en/33ff438ec37160a105f0700819a5b78a07918e1913fc2f249184b1f46a248e4e?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1745313271&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NTMxMzI3MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9IZWxzaW5raS1OTFAvb3B1cy1tdC1tdWwtZW4vMzNmZjQzOGVjMzcxNjBhMTA1ZjA3MDA4MTlhNWI3OGEwNzkxOGUxOTEzZmMyZjI0OTE4NGIxZjQ2YTI0OGU0ZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=E952aYE-vZ%7Eco1oqA40RLdBRst38MDngaLGP9u1RrdfHOd6--hBtd6UvohB6oQRXbX1LhADBZzZewHCGB5rvQuTYeS%7EnBkCUhiVAs00UCEcQ6gxVMEpfFSK-qzvmNq2g39LhK6xuMd6akIDFVahsBSLU5R5dM97Qezg0J5%7E3WbuRL534qfUdmANRXF4w55WTQbUyiDFkZMGsc3vjAQPvVTr2gNFDZuyGWt0wB-sNiHFGMnFilaYS2owHDtViGsYO-8Muefc0Z0tiOHz2FhTt4e6G%7E%7EnjGHU6TVOS75mxu0C%7EqYljhMa5qTSTciLIGMRw-Yp-yMYUWNN9UdtwXBxPWg__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "WARNING:huggingface_hub.file_download:Error while downloading from https://cdn-lfs.hf.co/Helsinki-NLP/opus-mt-mul-en/33ff438ec37160a105f0700819a5b78a07918e1913fc2f249184b1f46a248e4e?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1745313271&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NTMxMzI3MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9IZWxzaW5raS1OTFAvb3B1cy1tdC1tdWwtZW4vMzNmZjQzOGVjMzcxNjBhMTA1ZjA3MDA4MTlhNWI3OGEwNzkxOGUxOTEzZmMyZjI0OTE4NGIxZjQ2YTI0OGU0ZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=E952aYE-vZ%7Eco1oqA40RLdBRst38MDngaLGP9u1RrdfHOd6--hBtd6UvohB6oQRXbX1LhADBZzZewHCGB5rvQuTYeS%7EnBkCUhiVAs00UCEcQ6gxVMEpfFSK-qzvmNq2g39LhK6xuMd6akIDFVahsBSLU5R5dM97Qezg0J5%7E3WbuRL534qfUdmANRXF4w55WTQbUyiDFkZMGsc3vjAQPvVTr2gNFDZuyGWt0wB-sNiHFGMnFilaYS2owHDtViGsYO-8Muefc0Z0tiOHz2FhTt4e6G%7E%7EnjGHU6TVOS75mxu0C%7EqYljhMa5qTSTciLIGMRw-Yp-yMYUWNN9UdtwXBxPWg__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278fec3d92d8410a87249bd1e85058e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  61%|######    | 189M/310M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/Helsinki-NLP/opus-mt-mul-en/33ff438ec37160a105f0700819a5b78a07918e1913fc2f249184b1f46a248e4e?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1745313271&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NTMxMzI3MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9IZWxzaW5raS1OTFAvb3B1cy1tdC1tdWwtZW4vMzNmZjQzOGVjMzcxNjBhMTA1ZjA3MDA4MTlhNWI3OGEwNzkxOGUxOTEzZmMyZjI0OTE4NGIxZjQ2YTI0OGU0ZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=E952aYE-vZ%7Eco1oqA40RLdBRst38MDngaLGP9u1RrdfHOd6--hBtd6UvohB6oQRXbX1LhADBZzZewHCGB5rvQuTYeS%7EnBkCUhiVAs00UCEcQ6gxVMEpfFSK-qzvmNq2g39LhK6xuMd6akIDFVahsBSLU5R5dM97Qezg0J5%7E3WbuRL534qfUdmANRXF4w55WTQbUyiDFkZMGsc3vjAQPvVTr2gNFDZuyGWt0wB-sNiHFGMnFilaYS2owHDtViGsYO-8Muefc0Z0tiOHz2FhTt4e6G%7E%7EnjGHU6TVOS75mxu0C%7EqYljhMa5qTSTciLIGMRw-Yp-yMYUWNN9UdtwXBxPWg__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "WARNING:huggingface_hub.file_download:Error while downloading from https://cdn-lfs.hf.co/Helsinki-NLP/opus-mt-mul-en/33ff438ec37160a105f0700819a5b78a07918e1913fc2f249184b1f46a248e4e?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1745313271&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NTMxMzI3MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9IZWxzaW5raS1OTFAvb3B1cy1tdC1tdWwtZW4vMzNmZjQzOGVjMzcxNjBhMTA1ZjA3MDA4MTlhNWI3OGEwNzkxOGUxOTEzZmMyZjI0OTE4NGIxZjQ2YTI0OGU0ZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=E952aYE-vZ%7Eco1oqA40RLdBRst38MDngaLGP9u1RrdfHOd6--hBtd6UvohB6oQRXbX1LhADBZzZewHCGB5rvQuTYeS%7EnBkCUhiVAs00UCEcQ6gxVMEpfFSK-qzvmNq2g39LhK6xuMd6akIDFVahsBSLU5R5dM97Qezg0J5%7E3WbuRL534qfUdmANRXF4w55WTQbUyiDFkZMGsc3vjAQPvVTr2gNFDZuyGWt0wB-sNiHFGMnFilaYS2owHDtViGsYO-8Muefc0Z0tiOHz2FhTt4e6G%7E%7EnjGHU6TVOS75mxu0C%7EqYljhMa5qTSTciLIGMRw-Yp-yMYUWNN9UdtwXBxPWg__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c95f1a8b1494629b0f467d1b225c156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  61%|######    | 189M/310M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/Helsinki-NLP/opus-mt-mul-en/33ff438ec37160a105f0700819a5b78a07918e1913fc2f249184b1f46a248e4e?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1745313271&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NTMxMzI3MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9IZWxzaW5raS1OTFAvb3B1cy1tdC1tdWwtZW4vMzNmZjQzOGVjMzcxNjBhMTA1ZjA3MDA4MTlhNWI3OGEwNzkxOGUxOTEzZmMyZjI0OTE4NGIxZjQ2YTI0OGU0ZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=E952aYE-vZ%7Eco1oqA40RLdBRst38MDngaLGP9u1RrdfHOd6--hBtd6UvohB6oQRXbX1LhADBZzZewHCGB5rvQuTYeS%7EnBkCUhiVAs00UCEcQ6gxVMEpfFSK-qzvmNq2g39LhK6xuMd6akIDFVahsBSLU5R5dM97Qezg0J5%7E3WbuRL534qfUdmANRXF4w55WTQbUyiDFkZMGsc3vjAQPvVTr2gNFDZuyGWt0wB-sNiHFGMnFilaYS2owHDtViGsYO-8Muefc0Z0tiOHz2FhTt4e6G%7E%7EnjGHU6TVOS75mxu0C%7EqYljhMa5qTSTciLIGMRw-Yp-yMYUWNN9UdtwXBxPWg__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "WARNING:huggingface_hub.file_download:Error while downloading from https://cdn-lfs.hf.co/Helsinki-NLP/opus-mt-mul-en/33ff438ec37160a105f0700819a5b78a07918e1913fc2f249184b1f46a248e4e?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1745313271&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NTMxMzI3MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9IZWxzaW5raS1OTFAvb3B1cy1tdC1tdWwtZW4vMzNmZjQzOGVjMzcxNjBhMTA1ZjA3MDA4MTlhNWI3OGEwNzkxOGUxOTEzZmMyZjI0OTE4NGIxZjQ2YTI0OGU0ZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=E952aYE-vZ%7Eco1oqA40RLdBRst38MDngaLGP9u1RrdfHOd6--hBtd6UvohB6oQRXbX1LhADBZzZewHCGB5rvQuTYeS%7EnBkCUhiVAs00UCEcQ6gxVMEpfFSK-qzvmNq2g39LhK6xuMd6akIDFVahsBSLU5R5dM97Qezg0J5%7E3WbuRL534qfUdmANRXF4w55WTQbUyiDFkZMGsc3vjAQPvVTr2gNFDZuyGWt0wB-sNiHFGMnFilaYS2owHDtViGsYO-8Muefc0Z0tiOHz2FhTt4e6G%7E%7EnjGHU6TVOS75mxu0C%7EqYljhMa5qTSTciLIGMRw-Yp-yMYUWNN9UdtwXBxPWg__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83ae68db60041a386b7e1351f058bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  61%|######    | 189M/310M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06d426528e34e9090a88e261bf78b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "INFO:__main__:Helsinki-NLP model loaded successfully\n",
      "INFO:__main__:Google translator initialized successfully\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74fdf08e2d104479a7708e4a53d90987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/310M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original: days all the pickme drivers are bad\n",
      "Translated: Days all the pickme drivers are bad\n",
      "\n",
      "Original: hayar yanna ai baya mekai siddiya me hayar eka yaddi kalin giya hayar valata ganak PickMe ekata gewanna thiyanawa meka giyoth adu wela tmi labenne godak kattiya hayar ganne nethhe okai habai hamoma mehema ne e nisa okkotama dos kiyana eka weradi meka e riyadurage weraddak\n",
      "Translated: Position after which the pointer must stay as though the pointer were pushing the mouse pointer over the clock. Pickme and i have a lot of fun and i don't know what it's like. I don't know what's going on here. Like a dream of a dream, we're going to have a dream.\n",
      "\n",
      "Original: හයර් එක එයා approve කරන්න ඇත්තෙ cash ද card ද කියල බලල මෙයා මගදි තමයි card payment එකකට මාරු වෙලා තියෙන්නෙ ඒක අසාධාරණයි මෙයා කලින්ම card payment කියල තිබුනනම් මේ හයර් එක ත් ‍ රීවීල් එකේ කෙනා ගන්නෙ නෑ eken ගෙව්වවම ඒක යන්නෙ pick me එකට ඒ සල්ලි එයාට ලැබෙන්නෙ පස්සෙඅවුරුදු 6 ක් pick me app එකෙන් ගියානම් දන්නෙ නැද්ද කලින්ම cash ද card ද කියල දාන්න\n",
      "Translated: The hero he's actually trying to approve of is cash the card. He's between him and the card. It's not fair for him to say he's a card payment before. If this hero doesn't take anyone from reville, it's gonna pick me up when he pays for the eken. You don't know if he'll get that money years later when he's gone by pick me app? try moving card piles around\n",
      "\n",
      "Original: පියුමි නෙවි කාට උනත් මේක අසාදාරනයි මට හැමදාම මන් දන්න අයට හැමදාම වෙන දේපුදුම මලක් පනින්නේ මේ පික් මී අයට එයාලට company එකෙන් ගෙවන ගාන අඩුයි නම් කාඩ් වලින් පේ කරනවා නම් ලොකේශන් එකට එන්න කලින් එක්කෝ කියන්න ඕන අපිත් වාහනේ එනකන් බලන් ඉදලා ආවට පස්සේ තව රස්තියාදු කරලා කොච්චර අපිව පාරවල් වල බස්සල ගිහින් ඇත්ද මම කොච්චර අසරන වෙලා ඇද්ද උදේම ගමනක් ‍ යද්දි atm එකක් හොයන් යන්න ගියාමප් ‍ රශ්නේ වෙන කෙනෙක් තමයි ඒක දන්නේ\n",
      "Translated: It's not unusual for whoever doesn't have pium. I always know what's going on. A flower is cut off by this pick-me-me people if they have less pay from the company than they can see from the cards. If you do, we'll have to tell you before we get to the world and we'll be there until we get in the car. And then how much more are we going to get on the streets? how bad am i? i'm going to find another one when i'm on my way out of town. I know.\n",
      "\n",
      "Original: එකේ තියෙන්නෙත් මගෙ සල්ලි බන් වෙන උනගෙ නෙමේ අනික උබලට දවසක් ඇතුලත සල්ලි ලැබෙනවනෙ මන් uber දැනට ගියපු හැම හයර් එකක්ම card uber car වල drivers ල genuine pick me use karn naththe ekai\n",
      "Translated: It's not my money that's gonna get paid for you in one day. Every hero i've ever been to, uh, the drivers of the uber car's genine pick me use group not known\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from googletrans import Translator\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Global variables to avoid reloading models\n",
    "helsinki_translator = None\n",
    "google_translator = None\n",
    "\n",
    "# -------------------- Text Processing Functions --------------------\n",
    "\n",
    "def clean_sinhala_text(text):\n",
    "    \"\"\"Special preprocessing for Sinhala text\"\"\"\n",
    "    if not text or len(text) == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Handle mixed English-Sinhala text\n",
    "    # Extract English words/terms that should be preserved\n",
    "    english_terms = re.findall(r'[a-zA-Z]+', text)\n",
    "    \n",
    "    # Replace numbers and special characters with spaces\n",
    "    text = re.sub(r'[^\\u0D80-\\u0DFFa-zA-Z\\s.,!?]', ' ', text)\n",
    "    \n",
    "    # Clean up extra spaces again\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def post_process_translation(text):\n",
    "    \"\"\"Clean up translation results\"\"\"\n",
    "    if not text or len(text) == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    # Fix common translation artifacts\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r' \\.', '.', text)\n",
    "    text = re.sub(r' ,', ',', text)\n",
    "    \n",
    "    # Remove repetitive phrases (common in poor translations)\n",
    "    sentences = text.split('. ')\n",
    "    unique_sentences = []\n",
    "    for s in sentences:\n",
    "        if s and s not in unique_sentences:\n",
    "            unique_sentences.append(s)\n",
    "    \n",
    "    # Rejoin and capitalize\n",
    "    text = '. '.join(s.capitalize() for s in unique_sentences if s)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def chunk_text_advanced(text, max_length=100):\n",
    "    \"\"\"Break text into smaller chunks with better boundaries\"\"\"\n",
    "    if not text or len(text) <= max_length:\n",
    "        return [text]\n",
    "    \n",
    "    # Try to split on punctuation first\n",
    "    chunks = []\n",
    "    pattern = r'([።,.!?])'\n",
    "    sentences = re.split(pattern, text)\n",
    "    \n",
    "    current_chunk = \"\"\n",
    "    for i in range(0, len(sentences), 2):\n",
    "        sentence = sentences[i]\n",
    "        punctuation = sentences[i+1] if i+1 < len(sentences) else \"\"\n",
    "        \n",
    "        if len(current_chunk) + len(sentence) + len(punctuation) <= max_length:\n",
    "            current_chunk += sentence + punctuation\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            current_chunk = sentence + punctuation\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    # If no good sentence boundaries, try word boundaries\n",
    "    if not chunks or len(chunks) == 1 and len(chunks[0]) > max_length:\n",
    "        chunks = []\n",
    "        words = text.split()\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for word in words:\n",
    "            if len(current_chunk) + len(word) + 1 <= max_length:\n",
    "                current_chunk += \" \" + word if current_chunk else word\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk)\n",
    "                current_chunk = word\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "    \n",
    "    # Last resort: character-based chunking\n",
    "    if not chunks:\n",
    "        chunks = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# -------------------- Translation Functions --------------------\n",
    "\n",
    "def get_helsinki_translator():\n",
    "    \"\"\"Initialize Helsinki translator only once\"\"\"\n",
    "    global helsinki_translator\n",
    "    if helsinki_translator is None:\n",
    "        try:\n",
    "            logger.info(\"Loading Helsinki-NLP model...\")\n",
    "            # Use the multilingual model instead of the Sinhala-specific one\n",
    "            model_name = \"Helsinki-NLP/opus-mt-mul-en\"  # Multi-language to English model\n",
    "            \n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "            device = 0 if torch.cuda.is_available() else -1\n",
    "            \n",
    "            helsinki_translator = pipeline(\n",
    "                \"translation\", \n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                device=device\n",
    "            )\n",
    "            logger.info(\"Helsinki-NLP model loaded successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading Helsinki model: {str(e)}\")\n",
    "            try:\n",
    "                # Try with direct pipeline creation\n",
    "                helsinki_translator = pipeline(\n",
    "                    \"translation\", \n",
    "                    model=\"Helsinki-NLP/opus-mt-mul-en\",\n",
    "                    device=0 if torch.cuda.is_available() else -1\n",
    "                )\n",
    "                logger.info(\"Helsinki-NLP fallback model loaded successfully\")\n",
    "            except Exception as e2:\n",
    "                logger.error(f\"Error in fallback Helsinki initialization: {str(e2)}\")\n",
    "                logger.info(\"Will use Google Translate as fallback\")\n",
    "                helsinki_translator = None\n",
    "    \n",
    "    return helsinki_translator\n",
    "\n",
    "def translate_helsinki_improved(text):\n",
    "    \"\"\"Enhanced Helsinki translation with better preprocessing\"\"\"\n",
    "    try:\n",
    "        if not text or len(text) == 0:\n",
    "            return \"\"\n",
    "        \n",
    "        # Clean text specifically for Sinhala\n",
    "        text = clean_sinhala_text(text)\n",
    "        \n",
    "        # Get or initialize translator\n",
    "        translator = get_helsinki_translator()\n",
    "        if not translator:\n",
    "            logger.error(\"Helsinki translator not available\")\n",
    "            return \"\"\n",
    "        \n",
    "        # Break into smaller chunks for better quality\n",
    "        chunks = chunk_text_advanced(text, max_length=80)  # Smaller chunks for better quality\n",
    "        translated_chunks = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            try:\n",
    "                # Use temperature for slightly more fluent translations\n",
    "                result = translator(chunk, max_length=512, do_sample=True, temperature=0.7)[0]['translation_text']\n",
    "                translated_chunks.append(result)\n",
    "                # Small delay to avoid resource issues\n",
    "                time.sleep(0.2)\n",
    "            except Exception as chunk_error:\n",
    "                logger.error(f\"Error translating chunk: {str(chunk_error)}\")\n",
    "                # Try again with a simpler approach\n",
    "                try:\n",
    "                    result = translator(chunk, max_length=400)[0]['translation_text']\n",
    "                    translated_chunks.append(result)\n",
    "                except:\n",
    "                    # If all else fails, append an empty string to maintain chunk alignment\n",
    "                    translated_chunks.append(\"\")\n",
    "        \n",
    "        # Join and post-process\n",
    "        full_translation = ' '.join(chunk for chunk in translated_chunks if chunk)\n",
    "        return post_process_translation(full_translation)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Helsinki improved translation error: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def get_google_translator():\n",
    "    \"\"\"Initialize Google translator only once\"\"\"\n",
    "    global google_translator\n",
    "    if google_translator is None:\n",
    "        try:\n",
    "            google_translator = Translator()\n",
    "            logger.info(\"Google translator initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing Google translator: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    return google_translator\n",
    "\n",
    "def translate_google_improved(text):\n",
    "    \"\"\"Enhanced Google translation with better preprocessing\"\"\"\n",
    "    try:\n",
    "        if not text or len(text) == 0:\n",
    "            return \"\"\n",
    "        \n",
    "        # Clean text specifically for Sinhala\n",
    "        text = clean_sinhala_text(text)\n",
    "        \n",
    "        # Get or initialize translator\n",
    "        translator = get_google_translator()\n",
    "        if not translator:\n",
    "            logger.error(\"Failed to initialize Google translator\")\n",
    "            return \"\"\n",
    "        \n",
    "        # Break into smaller chunks for better quality\n",
    "        chunks = chunk_text_advanced(text, max_length=100)\n",
    "        translated_chunks = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            try:\n",
    "                # Random delay to avoid rate limiting\n",
    "                time.sleep(random.uniform(0.7, 2.0))\n",
    "                result = translator.translate(chunk, src='si', dest='en').text\n",
    "                translated_chunks.append(result)\n",
    "            except Exception as chunk_error:\n",
    "                logger.error(f\"Error translating chunk with Google: {str(chunk_error)}\")\n",
    "                # Try again after a longer delay\n",
    "                try:\n",
    "                    time.sleep(3.0)\n",
    "                    result = translator.translate(chunk, src='si', dest='en').text\n",
    "                    translated_chunks.append(result)\n",
    "                except:\n",
    "                    # If all else fails, append an empty string to maintain chunk alignment\n",
    "                    translated_chunks.append(\"\")\n",
    "        \n",
    "        # Join and post-process\n",
    "        full_translation = ' '.join(chunk for chunk in translated_chunks if chunk)\n",
    "        return post_process_translation(full_translation)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Google improved translation error: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def translate_improved(text, max_retries=3):\n",
    "    \"\"\"Try multiple translation approaches with better quality settings\"\"\"\n",
    "    if not text or len(text) == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    # Keep track of all successful translations\n",
    "    translations = []\n",
    "    \n",
    "    # Try Helsinki with improved settings if available\n",
    "    translator = get_helsinki_translator()\n",
    "    if translator:  # Only try if translator was successfully initialized\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                result = translate_helsinki_improved(text)\n",
    "                if result and len(result) > len(text)/4:  # Ensure meaningful translation\n",
    "                    translations.append(result)\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Helsinki attempt {attempt+1} failed: {str(e)}\")\n",
    "                time.sleep(1.5)\n",
    "    \n",
    "    # Try Google with improved settings\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = translate_google_improved(text)\n",
    "            if result and len(result) > len(text)/4:\n",
    "                translations.append(result)\n",
    "                break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Google attempt {attempt+1} failed: {str(e)}\")\n",
    "            time.sleep(2)\n",
    "    \n",
    "    # If at least one translation succeeded\n",
    "    if translations:\n",
    "        # Choose based on quality heuristics (currently using length as a proxy for completeness)\n",
    "        translations.sort(key=len, reverse=True)\n",
    "        return translations[0]\n",
    "    \n",
    "    return \"\"  # Return empty if all attempts fail\n",
    "\n",
    "# -------------------- Main Processing Function --------------------\n",
    "\n",
    "def translate_all_sequential(texts):\n",
    "    \"\"\"Process all texts sequentially for maximum reliability\"\"\"\n",
    "    results = []\n",
    "    for text in tqdm(texts, desc=\"Translating\"):\n",
    "        results.append(translate_improved(text))\n",
    "    return results\n",
    "\n",
    "# -------------------- Main Execution --------------------\n",
    "\n",
    "# Main execution section - for Jupyter notebook use\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load data\n",
    "        file_path = '/Volumes/KODAK/folder 02/language_translation/Language_translator/data/filtered_dataset.csv'\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check columns\n",
    "        print(\"Available columns:\", df.columns.tolist())\n",
    "        \n",
    "        # Sample data for testing - smaller sample to focus on quality\n",
    "        sample_size = min(5, len(df))\n",
    "        sample_df = df.sample(sample_size, random_state=42)\n",
    "        \n",
    "        print(\"Starting translation with focus on quality...\")\n",
    "        \n",
    "        # Identify the text column\n",
    "        text_column = 'Comment' if 'Comment' in df.columns else df.columns[0]\n",
    "        print(f\"Using text column: {text_column}\")\n",
    "        \n",
    "        # Use sequential processing for maximum reliability\n",
    "        translated_texts = translate_all_sequential(sample_df[text_column].tolist())\n",
    "        \n",
    "        # Add translations to the dataframe\n",
    "        sample_df['translated_text'] = translated_texts\n",
    "        \n",
    "        # Show results\n",
    "        for idx, row in sample_df.iterrows():\n",
    "            print(f\"\\nOriginal: {row[text_column]}\")\n",
    "            print(f\"Translated: {row['translated_text']}\")\n",
    "        \n",
    "        # Optional: Save results to CSV\n",
    "        # sample_df.to_csv('/Volumes/KODAK/folder 02/language_translation/Language_translator/data/translated_sample_improved.csv', index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d6ae0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
